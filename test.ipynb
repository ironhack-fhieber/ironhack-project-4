{
 "cells": [
  {
   "metadata": {
    "id": "13a685dcad2a020d"
   },
   "cell_type": "markdown",
   "source": [
    "# Project"
   ],
   "id": "13a685dcad2a020d"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "initial_id",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1730299598913,
     "user_tz": -60,
     "elapsed": 5156,
     "user": {
      "displayName": "Fabian Hieber",
      "userId": "06681858431281640040"
     }
    },
    "outputId": "f5cb1d1a-a005-4fd1-e84c-cd0c272f69aa",
    "ExecuteTime": {
     "end_time": "2024-10-30T23:10:00.116696Z",
     "start_time": "2024-10-30T23:09:44.719667Z"
    }
   },
   "source": "!pip install langchain langchain-community langchain_openai youtube-transcript-api langsmith faiss-cpu",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\hibi9\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.3.4)\n",
      "Requirement already satisfied: langchain-community in c:\\users\\hibi9\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.3.3)\n",
      "Requirement already satisfied: langchain_openai in c:\\users\\hibi9\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.2.3)\n",
      "Requirement already satisfied: youtube-transcript-api in c:\\users\\hibi9\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.6.2)\n",
      "Requirement already satisfied: langsmith in c:\\users\\hibi9\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.1.129)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\hibi9\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (1.9.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\hibi9\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\hibi9\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from langchain) (2.0.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\hibi9\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from langchain) (3.10.9)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\hibi9\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.12 in c:\\users\\hibi9\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from langchain) (0.3.13)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in c:\\users\\hibi9\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from langchain) (0.3.0)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\hibi9\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\hibi9\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from langchain) (2.9.2)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\hibi9\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\hibi9\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from langchain) (8.5.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\hibi9\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\hibi9\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from langchain-community) (2.6.0)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.52.0 in c:\\users\\hibi9\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from langchain_openai) (1.52.1)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in c:\\users\\hibi9\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from langchain_openai) (0.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\hibi9\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from langsmith) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\hibi9\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from langsmith) (3.10.7)\n",
      "Requirement already satisfied: packaging in c:\\users\\hibi9\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from faiss-cpu) (24.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\hibi9\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\hibi9\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\hibi9\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\hibi9\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\hibi9\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\hibi9\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.14.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\hibi9\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.23.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\hibi9\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\hibi9\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from httpx<1,>=0.23.0->langsmith) (4.6.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\hibi9\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from httpx<1,>=0.23.0->langsmith) (2020.12.5)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\hibi9\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from httpx<1,>=0.23.0->langsmith) (1.0.5)\n",
      "Requirement already satisfied: idna in c:\\users\\hibi9\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from httpx<1,>=0.23.0->langsmith) (2.10)\n",
      "Requirement already satisfied: sniffio in c:\\users\\hibi9\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from httpx<1,>=0.23.0->langsmith) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\hibi9\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith) (0.14.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\hibi9\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from langchain-core<0.4.0,>=0.3.12->langchain) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\hibi9\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from langchain-core<0.4.0,>=0.3.12->langchain) (4.12.2)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\hibi9\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from openai<2.0.0,>=1.52.0->langchain_openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\hibi9\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from openai<2.0.0,>=1.52.0->langchain_openai) (0.5.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\hibi9\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from openai<2.0.0,>=1.52.0->langchain_openai) (4.66.5)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\hibi9\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\hibi9\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\hibi9\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hibi9\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hibi9\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from requests<3,>=2->langchain) (2.2.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\hibi9\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\hibi9\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.9.11)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\hibi9\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith) (1.2.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\hibi9\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.12->langchain) (3.0.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hibi9\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tqdm>4->openai<2.0.0,>=1.52.0->langchain_openai) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\hibi9\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\hibi9\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "id": "e6e56b8c64edad1c"
   },
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "id": "e6e56b8c64edad1c"
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.callbacks import LangChainTracer\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langsmith import Client\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from youtube_transcript_api._errors import TranscriptsDisabled"
   ],
   "metadata": {
    "id": "ofW_2-hdNbYA",
    "ExecuteTime": {
     "end_time": "2024-10-30T23:39:05.981276Z",
     "start_time": "2024-10-30T23:39:05.963286Z"
    }
   },
   "id": "ofW_2-hdNbYA",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {
    "id": "dcaab1a5d1d5401e"
   },
   "cell_type": "markdown",
   "source": [
    "## Prework"
   ],
   "id": "dcaab1a5d1d5401e"
  },
  {
   "cell_type": "code",
   "source": [
    "# Is this workbook running on Google colab?\n",
    "COLAB = 'google.colab' in str(get_ipython())"
   ],
   "metadata": {
    "id": "YVoWorH6pU5O",
    "ExecuteTime": {
     "end_time": "2024-10-30T23:10:38.775077Z",
     "start_time": "2024-10-30T23:10:38.763680Z"
    }
   },
   "id": "YVoWorH6pU5O",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "id": "2961a4f1faeb1fc"
   },
   "cell_type": "markdown",
   "source": [
    "### Helper Methods"
   ],
   "id": "2961a4f1faeb1fc"
  },
  {
   "metadata": {
    "id": "312308f37349bd3f",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1730300225921,
     "user_tz": -60,
     "elapsed": 300,
     "user": {
      "displayName": "Fabian Hieber",
      "userId": "06681858431281640040"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-10-30T23:10:04.583581Z",
     "start_time": "2024-10-30T23:10:04.557018Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_video_title(video_id):\n",
    "    \"\"\"\n",
    "    Extracts the title of a YouTube video.\n",
    "\n",
    "    Args:\n",
    "        video_url (str): The URL of the YouTube video.\n",
    "\n",
    "    Returns:\n",
    "        str: The title of the YouTube video.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(requests.get(f\"https://www.youtube.com/watch?v={video_id}\").text, 'html.parser')\n",
    "    return soup.title.string.replace(\" - YouTube\", \"\").strip()"
   ],
   "id": "312308f37349bd3f",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "id": "844e7cb52e2234e1"
   },
   "cell_type": "markdown",
   "source": [
    "### Get Transcript and Title"
   ],
   "id": "844e7cb52e2234e1"
  },
  {
   "metadata": {
    "id": "f88084f792a15060",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1730301596909,
     "user_tz": -60,
     "elapsed": 293,
     "user": {
      "displayName": "Fabian Hieber",
      "userId": "06681858431281640040"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-10-30T23:10:04.631127Z",
     "start_time": "2024-10-30T23:10:04.617980Z"
    }
   },
   "cell_type": "code",
   "source": [
    "id = 'XEzRZ35urlk'\n",
    "#id = '6nJX5tbZzDo'\n",
    "chatter = 'Fabian'"
   ],
   "id": "f88084f792a15060",
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "video_title = get_video_title(id)\n",
    "languages = ['en', 'de', 'es', 'pt']\n",
    "try:\n",
    "    raw_transcript = YouTubeTranscriptApi.get_transcript(id, languages=languages)\n",
    "except TranscriptsDisabled:\n",
    "    proxies = {'http': 'http://94.186.213.73:7212',\n",
    "               'https': 'http://94.186.213.73:7212'}\n",
    "    raw_transcript = YouTubeTranscriptApi.get_transcript(id, languages=languages, proxies=proxies)"
   ],
   "metadata": {
    "id": "5JTGoL7UzhMt",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1730304244250,
     "user_tz": -60,
     "elapsed": 5360,
     "user": {
      "displayName": "Fabian Hieber",
      "userId": "06681858431281640040"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-10-30T23:10:06.496130Z",
     "start_time": "2024-10-30T23:10:04.666656Z"
    }
   },
   "id": "5JTGoL7UzhMt",
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": [
    "raw_transcript[0:5]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mh-J0U0-0H9m",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1730304247194,
     "user_tz": -60,
     "elapsed": 276,
     "user": {
      "displayName": "Fabian Hieber",
      "userId": "06681858431281640040"
     }
    },
    "outputId": "65ea1737-fb2a-4603-b977-5142ec117c0b",
    "ExecuteTime": {
     "end_time": "2024-10-30T23:10:06.554776Z",
     "start_time": "2024-10-30T23:10:06.526511Z"
    }
   },
   "id": "mh-J0U0-0H9m",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': '[Cheers and Applause].\\n>>WOMAN: Google’s ambitions in\\xa0',\n",
       "  'start': 0.0,\n",
       "  'duration': 1.52},\n",
       " {'text': 'artificial intelligence.\\n>>MAN: Google launches Gemini,\\xa0',\n",
       "  'start': 1.52,\n",
       "  'duration': 2.36},\n",
       " {'text': \"the generative AI.\\n>> And it's completely changing\\xa0\",\n",
       "  'start': 3.88,\n",
       "  'duration': 2.6},\n",
       " {'text': 'the way we work.\\n>> You know, a lot has happened\\xa0',\n",
       "  'start': 6.48,\n",
       "  'duration': 3.28},\n",
       " {'text': 'in a year.\\nThere have been new beginnings.\\xa0',\n",
       "  'start': 9.76,\n",
       "  'duration': 6.0}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": [
    "transcript = \" \".join([entry['text'] for entry in raw_transcript])"
   ],
   "metadata": {
    "id": "1I4Ny8b_GJQC",
    "ExecuteTime": {
     "end_time": "2024-10-30T23:10:06.865813Z",
     "start_time": "2024-10-30T23:10:06.853218Z"
    }
   },
   "id": "1I4Ny8b_GJQC",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "id": "9782c318e359d810",
    "outputId": "75f53e32-7017-4ee6-a623-0334d05abfec",
    "ExecuteTime": {
     "end_time": "2024-10-30T23:10:07.064834Z",
     "start_time": "2024-10-30T23:10:06.994233Z"
    }
   },
   "cell_type": "code",
   "source": [
    "transcript"
   ],
   "id": "9782c318e359d810",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[Cheers and Applause].\\n>>WOMAN: Google’s ambitions in\\xa0 artificial intelligence.\\n>>MAN: Google launches Gemini,\\xa0 the generative AI.\\n>> And it\\'s completely changing\\xa0 the way we work.\\n>> You know, a lot has happened\\xa0 in a year.\\nThere have been new beginnings.\\xa0 We found new ways to find new\\nWays to find new ideas.\\xa0 And new solutions to age-old problems.\\n>> Sorry about your shirt.\\xa0 We dreamt of things --\\n>> Never too old for a\\xa0 treehouse.\\n>> We trained for things.\\xa0 >> All right!\\nLet’s go go go! >> And learned about this thing.\\nWe found new paths, took the\\xa0 next step, and made the big leap.\\nCannon ball!\\xa0 We filled days like they were\\nweeks.\\xa0 And more happened in months,\\nthan has happened in years.\\xa0 >> Hey, free eggs.\\n>> Things got bigger,\\xa0\\xa0 like waaay\\nbigger.\\xa0 And it wasn’t all just for him,\\nor for her.\\xa0 It was for everyone. And you know what?\\xa0 We’re just getting started. >>SUNDAR PICHAI:\\xa0\\xa0Hi, everyone.\\nGood morning.\\xa0 [Cheers and Applause].\\nwelcome to Google I/O.\\xa0 It\\'s great to have all of you with us.\\nWe have a few thousand\\xa0 developers with us here today at\\nShoreline.\\xa0 Millions more are joining\\nvirtually around the world.\\xa0 Thanks to everyone for being\\nhere.\\xa0 For those of you who haven’t\\nseen I/O before, it’s basically\\xa0 Google’s version of the Eras\\nTour, but with fewer costume\\xa0 changes.\\n[Laughter].\\xa0 At Google, though, we are fully in our Gemini era.\\nBefore we get into it, I want to\\xa0 reflect on this moment we’re in.\\nWe’ve been investing in AI for\\xa0 more than a decade, and innovating\\xa0\\nat every layer of the stack:\\xa0 Research, product, infrastructure\\nWe’re going to talk about it all today.\\xa0 Still, we are in the early\\xa0\\ndays of the AI platform shift.\\xa0 We see so much opportunity ahead for creators,\\xa0\\nfor developers, for startups, for everyone.\\xa0 Helping to drive those opportunities\\xa0\\nis what our Gemini era is all about.\\xa0 So let’s get started.\\xa0 A year ago on this stage, we\\nfirst shared our plans for\\xa0 Gemini, a frontier model built\\nto be natively multimodal from\\xa0 the very beginning, that could\\nreason across text, images,\\xa0 video, code, and more.\\nIt’s a big step in turning any\\xa0 input into any output.\\nAn I/O for a new generation.\\xa0 Since then we introduced the\\nfirst Gemini models, our most\\xa0 capable yet.\\nThey demonstrated\\xa0 state-of-the-art performance on\\nevery multimodal benchmark.\\xa0 And that was just the beginning.\\nTwo months later, we introduced\\xa0 Gemini 1.5 Pro, delivering a big\\nbreakthrough in long context.\\xa0 It can run 1 million tokens in\\nproduction, consistently.\\xa0 More than any other large-scale\\nfoundation model yet.\\xa0 We want everyone to benefit from\\nwhat Gemini can do, so we’ve\\xa0 worked quickly to share these\\nadvances with all of you.\\xa0 Today, more than 1.5 million\\ndevelopers use Gemini models\\xa0 across our tools.\\nYou’re using it to debug code,\\xa0 get new insights, and build the\\nnext generation of AI\\xa0 applications.\\nWe’ve also been bringing\\xa0 Gemini’s breakthrough\\ncapabilities across our products\\xa0 in powerful ways.\\nWe’ll show examples today across\\xa0 Search, Photos, Workspace,\\nAndroid and more.\\xa0 Today, all of our 2-billion user\\nproducts use Gemini.\\xa0 And we’ve introduced new\\nexperiences, too, including on\\xa0 Mobile, where people can\\ninteract with Gemini directly\\xa0 through the app.\\nNow available on Android and\\xa0 iOS.\\nAnd through Gemini Advanced,\\xa0 which provides access to our\\nmost capable models.\\xa0 Over 1 million people have\\nsigned up to try it, in just\\xa0 three months.\\nAnd it continues to show strong\\xa0 momentum.\\nOne of the most exciting\\xa0 transformations with Gemini has\\nbeen in Google Search.\\xa0 In the past year, we’ve answered\\nbillions of queries as part of\\xa0 our Search Generative\\nExperience.\\xa0 People are using it to Search in\\nentirely new ways.\\xa0 And asking new types of\\nquestions, longer and more\\xa0 complex queries, even searching\\nwith photos, and getting back\\xa0 the best the web has to offer.\\nWe’ve been testing this\\xa0 experience outside of Labs, and\\nwe’re encouraged to see not only\\xa0 an increase in Search usage, but\\nalso an increase in user\\xa0 satisfaction.\\nI’m excited to announce that\\xa0 we’ll begin launching this fully\\nrevamped experience, AI\\xa0 Overviews, to everyone in the\\nU.S. this week.\\xa0 And we’ll bring it to more\\ncountries soon. [Cheers and Applause].\\nThere’s so much innovation\\xa0 happening in Search.\\nThanks to Gemini we can create\\xa0 much more powerful search\\nexperiences, including within\\xa0 our products.\\nLet me show you an example in\\xa0 Google Photos.\\nWe launched Google Photos almost\\xa0 nine years ago.\\nSince then, people have used it\\xa0 to organize their most important\\nmemories.\\xa0 Today that amounts to more than\\n6 billion photos and videos\\xa0 uploaded every single day.\\nAnd people love using Photos to\\xa0 search across their life.\\nWith Gemini, we’re making that a\\xa0 whole lot easier.\\nSay you’re at a parking station\\xa0 ready to pay, but you can’t\\nrecall your license plate\\xa0 number.\\nBefore, you could search Photos\\xa0 for keywords and then scroll\\nthrough years’ worth of photos,\\xa0 looking for the right one.\\nNow, you can simply ask Photos.\\xa0 It knows the cars that appear\\noften, it triangulates which one\\xa0 is yours, and just tells you\\xa0\\nthe license plate number. [Cheers and Applause].\\nAnd Ask Photos can help you\\xa0 search your memories in a deeper\\nway.\\xa0 For example, you might be\\nreminiscing about your daughter\\xa0 Lucia’s early milestones.\\nYou can ask photos, when did Lucia learn to swim?\\xa0 And you can follow up with\\nup with something more complex.\\xa0 Show me how Lucia\\'s swimming has progressed.\\nHere, Gemini goes beyond a\\xa0 simple search, recognizing\\ndifferent contexts from doing\\xa0 laps in the pool, to snorkeling\\nin the ocean, to the text and\\xa0 dates on her swimming\\ncertificates.\\xa0 And Photos packages it all up\\ntogether in a summary, so you\\xa0 can really take it all in, and\\nrelive amazing memories all over\\xa0 again.\\nWe’re rolling out Ask Photos\\xa0 this summer, with more\\ncapabilities to come. [Cheers and Applause].\\nUnlocking knowledge across\\xa0 formats is why we built Gemini\\nto be multimodal from the ground\\xa0 up.\\nIt’s one model, with all the\\xa0 modalities built in.\\nSo not only does it understand\\xa0 each type of input, it finds\\nconnections between them.\\xa0 Multimodality radically expands\\nthe questions we can ask, and\\xa0 the answers we will get back.\\nLong context takes this a step\\xa0 further, enabling us to bring in\\neven more information, hundreds\\xa0 of pages of text, hours of\\naudio, a full hour of video, or\\xa0 entire code repos.\\nOr, if you want, roughly 96\\xa0 Cheesecake Factory menus.\\n[Laughter].\\xa0 For that many menus, you’d need\\na one million token context\\xa0 window, now possible with Gemini\\n1.5 Pro.\\xa0 Developers have been using it in\\nsuper interesting ways.\\xa0 Let’s take a look.\\n>> I remember the announcement,\\xa0 the 1 million token context\\nwindow, and my first reaction\\xa0 was there\\'s no way they were\\nable to achieve this.\\xa0 >> I wanted to test its\\ntechnical skills, so I uploaded\\xa0 a line chart.\\nIt was temperatures between like\\xa0 Tokyo and Berlin and how they\\nwere across the 12 months of the\\xa0 year.\\n>> So\\xa0\\xa0 I got in there and I threw\\nin the Python library that was\\xa0 really struggling with and I\\njust asked it a simple question.\\xa0 And it nailed it.\\nIt could find specific\\xa0 references to comments in the\\ncode and specific requests that\\xa0 people had made and other issues\\nthat people had had, but then\\xa0 suggest a fix for it that\\nrelated to what I was working\\xa0 on.\\n>> I immediately tried to kind\\xa0 of crash it.\\nSo I took, you know, four or\\xa0 five research papers I had on my\\ndesktop, and it\\'s a mind-blowing\\xa0 experience when you add so much\\ntext, and then you see the kind\\xa0 of amount of tokens you add is\\nnot even at half the capacity.\\xa0 >> It felt a little bit like\\nChristmas because you saw things\\xa0 kind of peppered up to the top\\nof your feed about, like, oh,\\xa0 wow, I built this thing, or oh,\\nit\\'s doing this, and I would\\xa0 have never expected.\\n>> Can I shoot a video of my\\xa0 possessions and turn that into a\\nsearchable database?\\xa0 So I ran to my bookshelf, and I\\nshot video just panning my\\xa0 camera along the bookshelf and I\\nfed the video into the model.\\xa0 It gave me the titles and\\nauthors of the books, even\\xa0 though the authors weren\\'t\\nvisible on those book spines,\\xa0 and on the bookshelf there was a\\nsquirrel nut cracker sat in\\xa0 front of the book, truncating\\nthe title.\\xa0 You could just see the word\\n\"sightsee\", and it still guessed\\xa0 the correct book.\\nThe range of things you can do\\xa0 with that is almost unlimited.\\n>> So at that point for me was\\xa0 just like a click, like, this is\\nit.\\xa0 I thought, like, I had like\\xa0\\na super power in my hands.\\xa0 >> It was poetry.\\nIt was beautiful.\\xa0 I was so happy!\\nThis is going to be amazing!\\xa0 This is going to help people!\\n>> This is kind of where the\\xa0 future of language models are\\ngoing.\\xa0 Personalized to you, not because\\nyou trained it to be personal to\\xa0 you, but personal to you because\\nyou can give it such a vast\\xa0 understanding of who you are.\\n[Applause].\\xa0 >>SUNDAR PICHAI: We’ve been\\nrolling out Gemini 1.5 Pro with\\xa0 long context in preview over the\\nlast few months.\\xa0 We’ve made a series of quality\\nimprovements across translation,\\xa0 coding, and reasoning.\\nYou’ll see these updates\\xa0 reflected in the model starting\\ntoday.\\xa0 I\\'m excited to announce that\\nwe’re bringing this improved\\xa0 version of Gemini 1.5 Pro to all\\ndevelopers globally. [Cheers and Applause].\\nIn addition, today Gemini 1.5\\xa0 Pro with 1 million context is now directly\\xa0\\navailable for consumers in Gemini Advanced,\\xa0\\xa0 and can be used across 35 languages.\\nOne million tokens is opening up\\xa0 entirely new possibilities.\\nIt’s exciting, but I think we\\xa0 can push ourselves even further.\\nSo today, we are expanding the\\xa0 context window to 2 million\\nTokens. [Cheers and Applause].\\nWe are making it available\\xa0\\xa0 for developers in private preview.\\nIt\\'s amazing to look back and\\xa0 see just how much progress we\\'ve\\nmade in a few months.\\xa0 This represents the next step on our journey\\xa0\\ntowards the ultimate goal of infinite context.\\xa0 Okay.\\nSo far,\\xa0\\xa0 we’ve talked about two\\ntechnical advances:\\xa0 multimodality and long context.\\nEach is powerful on its own.\\xa0 But together, they unlock deeper\\ncapabilities, and more\\xa0 intelligence.\\nLet’s see how this comes to life\\xa0 with Google Workspace.\\nPeople are always searching\\xa0 their emails in Gmail.\\nWe are working to make it much\\xa0 more powerful with Gemini.\\nLet’s look at how.\\xa0 As a parent, you want to know\\neverything that’s going on with\\xa0 your child’s school.\\nOkay, maybe not everything, but\\xa0 you want to stay informed.\\nGemini can help you keep up.\\xa0 Now we can ask Gemini to\\nsummarize all recent emails from\\xa0 the school.\\nIn the background, it’s\\xa0 identifying relevant emails, and\\neven analyzing attachments, like\\xa0 PDFs.\\nAnd you get a summary of\\xa0\\xa0 the key points and action items.\\nSo helpful.\\xa0 Maybe you were traveling this\\nweek and couldn’t make the PTA\\xa0 meeting.\\nThe recording of the meeting is\\xa0 an hour long.\\nIf it’s from Google Meet, you\\xa0 can ask Gemini to give you the\\nhighlights. [Cheers and Applause].\\nThere’s a parents group looking\\xa0 for volunteers, and you’re free\\nthat day.\\xa0 So of course, Gemini can draft a\\nreply.\\xa0 There are countless other\\nexamples of how this can make\\xa0 life easier.\\nGemini 1.5 Pro is available\\xa0 today in Workspace Labs.\\nAparna will share more later on. [Applause].\\nWe just looked at an example with text outputs.\\xa0 But with a multimodal model, we\\ncan do so much more.\\xa0 To show you an early demo of an\\naudio output in NotebookLM,\\xa0 here’s Josh.\\n>>JOSH WOODWARD: Hi, everyone!\\xa0 Last year, at I/O, we introduced\\nNotebook LM, a research and\\xa0 writing tool grounded in the\\ninformation you give it.\\xa0 Since then, we\\'ve seen a lot of\\nmomentum with students and\\xa0 teachers using it.\\nAnd today, Gemini 1.5 Pro is\\xa0 coming to Notebook LM, and it\\'s\\ngreat.\\xa0 Let me show you.\\nSo here we are in Notebook LM.\\xa0 You can load it up with all of\\nthe materials here on the left.\\xa0 In this notebook, I\\'ve been\\nusing it with my younger son,\\xa0\\xa0 and I\\'ve added some of his science worksheets,\\xa0\\na few slide decks from the teacher, and even an\\xa0\\xa0 open source textbook full of charts and diagrams.\\nWith 1.5 Pro, it instantly creates this notebook\\xa0\\xa0 guide with a helpful summary and can generate\\xa0\\na study guide, an FAQ, or even quizzes.\\xa0 But for my son Jimmy, he really learns\\xa0\\nbest when he can listen to something.\\xa0 So we\\'ve prototyped a new feature with\\xa0\\nGemini, and it\\'s called Audio Overviews.\\xa0 Notebook LM is going to use all\\nof the materials on the left\\xa0\\xa0 as input and output them into a lively\\xa0\\nscience discussion, personalized for him.\\xa0 Let\\'s take a listen.\\n>> So let\\'s dive into physics.\\xa0 What\\'s on deck for today?\\n>> Well, we\\'re starting with the\\xa0 basics.\\nForce in motion.\\xa0 >> Okay.\\n>> And that, of course, means we\\xa0 have to talk about Sir Isaac\\nNewton and his three laws of\\xa0 motion.\\n>> Ah,\\xa0\\xa0 yes, the foundation --\\nFor understanding how objects move and interact. >>JOSH WOODWARD: Oh, yes, this is\\xa0\\nwhere multimodal really shines.\\xa0 Now, it generated this audio\\xa0\\ndiscussion based on that text material.\\xa0 And what\\'s amazing is that my son\\xa0\\nand I can join into the conversation.\\xa0 And steer it whichever direction we want.\\xa0 When I tap \"join.\"\\n>> Hold on.\\xa0 We have a question.\\nWhat\\'s up, Josh?\\xa0 >>JOSH WOODWARD: Yeah, can you\\ngive my son Jimmy a basketball\\xa0 example?\\n>> Hey, Jimmy!\\xa0 That\\'s a fantastic idea!\\nBasketball is actually a great\\xa0 way to visualize force in\\nmotion.\\xa0 Let\\'s break it down.\\n>> Okay.\\xa0 So first, imagine a basketball\\njust sitting there on the court.\\xa0 It\\'s not moving, right?\\xa0 That\\'s because all the forces\\nacting on it are balanced.\\xa0 The downward pull of gravity --\\n>>JOSH WOODWARD: Pretty cool, right? [Cheers and Applause].\\nI gotta say, the first\\xa0\\xa0 time my son heard this, you should\\xa0\\nhave seen how big his eyes got!\\xa0 Because he was gripped.\\nThey were talking to him.\\xa0 He was learning science through\\xa0\\nthe example of basketball.\\xa0 His favorite sport.\\nNow, what\\'s interesting\\xa0\\xa0 is under the hood, you saw that Gemini had used\\xa0\\nsome of the concepts of gravity, Sir Isaac Newton,\\xa0\\xa0 but nothing in there was about basketball.\\nIt connected the dots and created that\\xa0\\xa0 age-appropriate example for him.\\nAnd this is what\\'s becoming\\xa0\\xa0 possible with the power of Gemini.\\nYou can give it lots of information in\\xa0\\xa0 any format, and it can be transformed in a way\\xa0\\nthat\\'s personalized and interactive for you.\\xa0 Back to you, Sundar.\\n[Applause].\\xa0 >>SUNDAR PICHAI: Thanks, Josh.\\nThe demo shows the real\\xa0 opportunity with multimodality.\\nSoon you’ll be able to mix and\\xa0 match inputs and outputs.\\nThis is what we mean when we say\\xa0 it’s an I/O for a new\\ngeneration.\\xa0 And I can see you all out there\\nthinking about the\\xa0 possibilities.\\nBut what if we could go even\\xa0 further?\\nThat’s one of the opportunities\\xa0 we see with AI agents.\\nLet me take a step back and\\xa0 explain what I mean by that.\\nI think about them as\\xa0 intelligent systems that show\\nreasoning, planning, and memory.\\xa0 Are able to “think” multiple\\nsteps ahead, work across\\xa0 software and systems, all to get\\nsomething done on your behalf,\\xa0 and most importantly, under your\\nsupervision.\\xa0 We are still in the early days,\\nand you’ll see glimpses of our\\xa0 approach throughout the day, but\\nlet me show you the kinds of use\\xa0 cases we are working hard to solve.\\nLet’s talk about shopping.\\xa0 It’s pretty fun to shop for\\nshoes, and a lot less fun to\\xa0 return them when they don’t fit.\\nImagine if Gemini could do all\\xa0 the steps for you: Searching\\nyour inbox for the receipt,\\xa0 locating the order number from\\nyour email, filling out a return\\xa0 form, and even scheduling a pickup.\\nThat\\'s much easier, right? [Applause].\\nLet’s take another example\\xa0 that’s a bit more complex.\\nSay you just moved to Chicago.\\xa0 You can imagine Gemini and\\nChrome working together to help\\xa0 you do a number of things to get\\nready: Organizing, reasoning,\\xa0 synthesizing on your behalf.\\nFor example, you’ll want to\\xa0 explore the city and find\\nservices nearby, from\\xa0 dry-cleaners to dog-walkers.\\nYou will have to update your new\\xa0\\xa0 address across dozens of Web sites.\\nGemini can work across these\\xa0 tasks and will prompt you for\\nmore information when needed, so\\xa0 you are always in control.\\nThat part is really important.\\xa0 as we prototype these experiences.\\nWe are thinking hard about how to do it in a way\\xa0\\xa0 that\\'s private, secure and works for everyone.\\nThese are simple-use cases, but\\xa0 they give you a good sense of\\nthe types of problems we want to\\xa0 solve, by building intelligent\\nsystems that think ahead,\\xa0 reason, and plan, all on your\\nbehalf.\\xa0 The power of Gemini, with\\nmultimodality, long context and\\xa0 agents, brings us closer to our\\nultimate goal: Making AI helpful\\xa0 for everyone.\\nWe see this as how we will make\\xa0\\xa0 the most progress against our mission.\\nOrganizing the world’s\\xa0 information across every input,\\nmaking it accessible via any\\xa0 output, and combining the\\nworld’s information with the\\xa0 information in your world in a\\nway that’s truly useful for you.\\xa0 To fully realize the benefits of\\nAI, we will continue to break\\xa0 new ground.\\nGoogle DeepMind is hard at work\\xa0 on this.\\nTo share more, please welcome,\\xa0 for the first time on the I/O\\nstage, Sir Demis. [Applause].\\n>>DEMIS HASSABIS:\\xa0\\xa0 Thanks,\\nSundar.\\xa0 It\\'s so great to be here.\\nEver since I was a kid, playing\\xa0 chess for the England Junior\\nTeam, I’ve been thinking about\\xa0 the nature of intelligence.\\nI was captivated by the idea of\\xa0 a computer that could think like\\na person.\\xa0 It’s ultimately why I became a\\nprogrammer and studied\\xa0 neuroscience.\\nI co-founded DeepMind in 2010\\xa0 with the goal of one day\\nbuilding AGI: Artificial general\\xa0 intelligence, a system that has\\nhuman-level cognitive\\xa0 capabilities.\\nI’ve always believed that if we\\xa0 could build this technology\\nresponsibly, its impact would be\\xa0 truly profound and it could\\nbenefit humanity in incredible\\xa0 ways.\\nLast year,\\xa0\\xa0 we reached a milestone on that path when we\\xa0\\nformed Google DeepMind, combining AI talent\\xa0\\xa0 from across the company in to one super unit.\\nSince then, we\\'ve built AI systems that can\\xa0\\xa0 do an amazing range of things, from turning\\xa0\\nlanguage and vision into action for robots,\\xa0\\xa0 navigating complex virtual environments, involving\\xa0\\nOlympiad level math problems, and even discovering\\xa0\\xa0 thousands of new materials.\\nJust last week, we announced\\xa0\\xa0 our next generation AlphaFold model.\\nIt can predict the structure and interactions\\xa0\\xa0 of nearly all of life\\'s molecules, including how\\xa0\\nproteins interact with strands of DNA and RNA.\\xa0 This will accelerate vitally important\\xa0\\nbiological and medical research from\\xa0\\xa0 disease understanding to drug discovery.\\nAnd all of this was made possible with the\\xa0\\xa0 best infrastructure for the AI era, including\\xa0\\nour highly optimized tensor processing units.\\xa0 At the center of our efforts is our Gemini model.\\nIt\\'s built up from the ground up to be natively\\xa0\\xa0 multimodal because that\\'s how we interact\\xa0\\nwith and understand the world around us.\\xa0 We\\'ve built a variety of\\xa0\\nmodels for different use cases.\\xa0 We\\'ve seen how powerful Gemini 1.5 Pro is,\\xa0\\nbut we also know from user feedback that some\\xa0 applications need lower latency\\nand a lower cost to serve.\\xa0 So today we’re introducing\\nGemini 1.5 Flash. [Cheers and Applause].\\nFlash is a lighter-weight model\\xa0 compared to Pro.\\nIt’s designed to be fast and\\xa0 cost-efficient to serve at\\nscale, while still featuring\\xa0 multimodal reasoning\\ncapabilities and breakthrough\\xa0 long context.\\nFlash is optimized for tasks\\xa0 where low latency and efficiency\\nmatter most.\\xa0 Starting today, you can use 1.5\\nFlash and 1.5 Pro with up to one\\xa0 million tokens in Google AI\\nStudio and Vertex AI.\\xa0 And developers can sign up to\\ntry two million tokens.\\xa0 We’re so excited to see what all\\nof you will create with it.\\xa0 And you\\'ll hear a little more\\xa0\\nabout Flash later on from Josh.\\xa0 We’re very excited by the\\nprogress we’ve made so far with\\xa0 our family of Gemini models.\\nBut we’re always striving to\\xa0 push the state-of-the-art even\\nfurther.\\xa0 At any one time we have many\\ndifferent models in training.\\xa0 And we use our very large and\\npowerful ones to help teach and\\xa0 train our production-ready\\nmodels.\\xa0 Together with user feedback,\\nthis cutting-edge research will\\xa0 help us to build amazing new\\nproducts for billions of people.\\xa0 For example, in December, we\\nshared a glimpse into the future\\xa0 of how people would interact\\nwith multimodal AI, and how this\\xa0 would end up powering a new set\\nof transformative experiences.\\xa0 Today, we have some exciting new\\nprogress to share about the\\xa0 future of AI assistants that\\nwe’re calling Project Astra. [Cheers and Applause].\\nFor a long time, we’ve wanted to\\xa0 build a universal AI agent that\\ncan be truly helpful in everyday\\xa0 life.\\nOur work making this vision a\\xa0 reality goes back many years.\\nIt\\'s why we made Gemini multimodal\\xa0\\xa0 from the very beginning.\\nAn agent like this has to\\xa0 understand and respond to our\\ncomplex and dynamic world just\\xa0 like we do.\\nIt would need to take in and\\xa0 remember what it sees so it can\\nunderstand context and take\\xa0 action.\\nAnd it would have to be\\xa0 proactive, teachable and\\npersonal, so you can talk to it\\xa0 naturally, without lag or delay.\\nWhile we’ve made great strides\\xa0 in developing AI systems that\\ncan understand multimodal\\xa0 information, getting response\\ntime down to something\\xa0 conversational is a difficult\\nengineering challenge.\\xa0 Building on our Gemini model,\\nwe’ve developed agents that can\\xa0 process information faster by\\ncontinuously encoding video\\xa0 frames, combining the video and\\nspeech input into a timeline of\\xa0 events, and caching this for\\nefficient recall.\\xa0 We’ve also enhanced how they\\nsound, with a wider range of\\xa0 intonations.\\nThese agents better understand\\xa0 the context you’re in, and can\\nrespond quickly in conversation,\\xa0 making the pace and quality of\\ninteraction feel much more\\xa0 natural.\\nHere’s a video of our prototype,\\xa0 which you’ll see has two parts.\\nEach part was captured in a\\xa0 single take, in real time.\\n>> Okay. Let\\'s do some tests.\\xa0 Tell me when you see something\\nthat makes sound.\\xa0 >> I see a speaker, which makes\\nsound.\\xa0 >> What is that part of the\\nspeaker called?\\xa0 >> That is the tweeter.\\nIt produces high frequency\\xa0 sounds.\\n>> Give me a creative\\xa0 alliteration about these.\\n>> Creative crayons color\\xa0 cheerfully.\\nThey certainly craft colorful\\xa0 creations.\\n>> What does that part of the\\xa0 code do?\\n>> This code defines encryption\\xa0 and decryption functions.\\nIt seems to use AES-CBC\\xa0 encryption to encode and decode\\ndata based on a key and an\\xa0 initialization vector (IV).\\n>> That\\'s right.\\xa0 What neighborhood do you think\\nI\\'m in?\\xa0 >> This appears to be the Kings\\nCross area of London.\\xa0 It is known for its railway\\nstation and transportation\\xa0 connections.\\n>> Do you remember where you saw\\xa0 my glasses?\\n>> Yes, I do.\\xa0 Your glasses were on the desk\\nnear a red apple. [Applause].\\n>> What can I add here to make\\xa0 this system faster?\\n>>\\xa0\\xa0 Adding a cache between the\\nserver and database could\\xa0 improve speed.\\n>> What does this remind you of?\\xa0 >> Schroedinger\\'s cat.\\n>> All right.\\xa0 Give me a band name for\\nthisduo.\\xa0 >> Golden Stripes.\\n>> Nice. Thanks, Gemini. [Applause].\\n>>DEMIS HASSABIS:\\xa0\\xa0 I think you\\'ll\\nagree it\\'s amazing to see how\\xa0 far AI has come, especially when\\nit comes to spatial\\xa0 understanding, video processing\\nand memory.\\xa0 It’s easy to envisage a future\\nwhere you can have an expert\\xa0 assistant by your side through\\nyour phone or new exciting form\\xa0 factors like glasses.\\nSome of these agent capabilities\\xa0 will come to Google products\\nlike the Gemini app later this\\xa0 year.\\nFor those of you onsite today,\\xa0 you can try out a live demo\\nversion of this experience in\\xa0 the AI Sandbox area.\\n \\n[Cheers and Applause].\\xa0 Next, let’s take a look at how\\nour innovations are helping\\xa0 people bring new creative ideas\\nto life.\\xa0 Today, we’re introducing a\\nseries of updates across our\\xa0 generative media tools with new\\nmodels covering image, music and\\xa0 video.\\nOver the past year, we’ve been\\xa0 enhancing quality, improving\\xa0\\nsafety and increasing access.\\xa0 To help tell this story, here’s\\nDoug.\\xa0 [Applause].\\n>>DOUG ECK: Thanks, Demis.\\xa0 Over the past few\\nmonths, we’ve been working hard\\xa0 to build a new image generation\\nmodel from the ground up, with\\xa0 stronger evaluations, extensive\\nred teaming, and\\xa0 state-of-the-art watermarking\\nwith SynthID.\\xa0 Today, I’m so excited to\\nintroduce Imagen 3.\\xa0 It’s our most capable image\\ngeneration model yet.\\xa0 Imagen 3 is more\\nphotorealistic.\\xa0 You can literally count the\\nwhiskers on its snout.\\xa0 With richer details, like the\\nincredible sunlight in this\\xa0 shot, and fewer visual artifacts\\nor distorted images.\\xa0 It understands prompts written\\nthe way people write.\\xa0 The more creative and detailed\\nyou are, the better.\\xa0 And Imagen 3 remembers to\\nincorporate small details like\\xa0 the ‘wildflowers’ or ‘a small\\nblue bird’ in this longer\\xa0 prompt.\\nPlus, this is our best model yet\\xa0 for rendering text, which has\\nbeen a challenge for image\\xa0 generation models.\\nIn side-by-side comparisons,\\xa0 independent evaluators\\npreferred Imagen 3 over other\\xa0\\xa0 popular image generation models.\\nIn sum, Imagen 3 is our\\xa0 highest-quality image generation\\nmodel so far.\\xa0 You can sign up today to try\\nImagen 3 in ImageFX, part of our\\xa0 suite of AI tools at\\nlabs.Google, and it will be\\xa0 coming soon to developers and\\nenterprise customers in Vertex\\xa0 AI.\\nAnother area, full of creative\\xa0 possibility, is generative\\nmusic.\\xa0 I’ve been working in this space\\nfor over 20 years and this has\\xa0 by far the most exciting year of my career.\\nWe’re exploring ways of working\\xa0 with artists to expand their\\ncreativity with AI.\\xa0 Together with YouTube, we’ve\\nbeen building Music AI Sandbox,\\xa0 a suite of professional music\\nAI tools that can create new\\xa0\\xa0 instrumental sections from scratch,\\xa0\\ntransfer styles between tracks, and more.\\xa0 To help us design and test them,\\nwe’ve been working closely with\\xa0 incredible musicians,\\nsongwriters and producers.\\xa0 Some of them made even entirely new\\xa0\\nsongs in ways that would not have been\\xa0\\xa0 possible without these tools.\\nLet’s hear from some of the\\xa0 artists we’ve been working with.\\n>>\\xa0\\xa0 I\\'m going to put this right\\nback into the Music AI tool.\\xa0 The same Boom, boom, bam, boom,\\nboom.\\xa0 What happens if Haiti meets\\nBrazil?\\xa0 Dude, I have no clue what\\'s\\nabout to be sprat out.\\xa0 This is what excites me.\\nDa da See see see.\\xa0 As a hip hop producer, we dug in\\nthe crates.\\xa0 We playin’ these vinyls, and the\\npart where there\\'s no vocal, we\\xa0 pull it, we sample it, and we\\ncreate an entire song around\\xa0 that.\\nSo right now we digging in the\\xa0 infinite crate.\\nIt’s endless.\\xa0 Where I found the AI really useful for me, this\\xa0\\nway to like fill in the sparser sort of elements\\xa0\\xa0 of my loops.\\nOkay.\\xa0 Let\\'s try bongos.\\nWe\\'re going to putviola.\\xa0 We\\'re going to put rhythmic\\nclapping, and we\\'re going to see\\xa0 what happens there.\\nOh, and it makes it sound,\\xa0 ironically, at the end of the\\nday, a little more human.\\xa0 So then this is entirely\\nGoogle\\'s loops right here.\\xa0 These are Gloops.\\nSo it\\'s like having, like, this\\xa0 weird friend that\\'s just like,\\xa0 try this, try that.\\nAnd then you\\'re like, Oh, okay.\\xa0 Yeah.\\nNo, that\\'s pretty dope.\\xa0 (indistinct noises)\\n>> The tools are capable of\\xa0 speeding up the process of\\nwhat\\'s in my head, getting it\\xa0 out.\\nYou\\'re able to move lightspeed\\xa0 with your creativity.\\nThis is amazing.\\xa0 That right there.\\n[Applause].\\xa0 >>DEMIS HASSABIS: I think this\\nreally shows what’s possible\\xa0 when we work with the artist\\ncommunity on the future of\\xa0 music.\\nYou can find some brand new\\xa0 songs from these acclaimed\\nartists and songwriters on their\\xa0 YouTube channels now.\\nThere\\'s one more area I\\'m\\xa0\\xa0 really excited to share with you.\\nOur teams have made some\\xa0 incredible progress in\\ngenerative video.\\xa0 Today, I’m excited to announce\\nour newest, most capable\\xa0 generative video model, called\\nVeo.\\xa0 [Cheers and Applause].\\nVeo creates high-quality, 1080P\\xa0 videos from text, image and\\nvideo prompts.\\xa0 It can capture the details of\\nyour instructions in different\\xa0 visual and cinematic styles.\\nYou can prompt for things like\\xa0 aerial shots of a landscape or a\\ntime lapse, and further edit\\xa0 your videos using additional\\nprompts.\\xa0 You can use Veo in our new\\nexperimental tool called\\xa0 VideoFX.\\nWe’re exploring features like\\xa0 storyboarding and generating\\nlonger scenes.\\xa0 Veo gives you unprecedented\\ncreative control.\\xa0 Techniques for generating static\\nimages have come a long way.\\xa0 But generating video is a\\ndifferent challenge altogether.\\xa0 Not only is it important to\\nunderstand where an object or\\xa0 subject should be in space, it\\nneeds to maintain this\\xa0 consistency over time, just like\\nthe car in this video.\\xa0 Veo builds upon years of our\\npioneering generative video\\xa0 model work, including GQN,\\nPhenaki, Walt, VideoPoet,\\xa0 Lumiere and much more.\\nWe combined the best of these\\xa0 architectures and techniques to\\nimprove consistency, quality and\\xa0 output resolution.\\nTo see what Veo can do, we put\\xa0 it in the hands of an amazing\\nfilmmaker.\\xa0 Let’s take a look.\\n>>DONALD GLOVER: Well, I\\'ve been\\xa0 interested in AI for a couple of\\nyears now.\\xa0 We got in contact with some of\\nthe people at Google, and they\\xa0 had been working on something of\\ntheir own.\\xa0 So we\\'re all meeting here at\\nGilga Farms to make a short\\xa0 film.\\n>>KORY MATHEWSON: The core\\xa0 technology is Google Deep Mind’s\\xa0 generative video model that has\\nbeen trained to convert input\\xa0 text into output video.\\n[Laughter].\\xa0 >>DONALD GLOVER: It looks good.\\n>>KORY MATHEWSON: We are able to\\xa0 bring ideas to life that were\\notherwise not possible.\\xa0 We can visualize things of time\\nscale that’s 10 or 100 times\\xa0 faster than before.\\n>>MATTHIEU KIM LORRAIN: When\\xa0 you\\'re shooting, you can\\'t\\nreally iterate as much as you\\xa0 wish.\\nAnd so we\\'ve been hearing the\\xa0 feedback that it allows for more\\noptionality, more iteration,\\xa0 more improvisation.\\n>>DONALD GLOVER: But that\\'s\\xa0 what\\'s cool about it.\\nIt\\'s like you can make a mistake\\xa0 faster.\\nThat\\'s all you really want at\\xa0 the end of the day, at least in\\nart, is just to make mistakes\\xa0 fast.\\n>>KORY MATHEWSON: So, using\\xa0 Gemini’s multimodal capabilities\\nto optimize the model’s training\\xa0 process, VEO is better able to\\ncapture the nuance from prompts.\\xa0 So this includes cinematic\\ntechniques and visual effects,\\xa0 giving you total creative\\xa0 control.\\n>>DONALD GLOVER: Everybody\\'s\\xa0 going to become a director and\\neverybody should be a director.\\xa0 Because at the heart of all of\\nthis is just storytelling.\\xa0 The closer we are to being able\\nto tell each other our stories,\\xa0 the more we will understand each\\nother.\\xa0 >>KORY MATHEWSON: These models\\nare really enabling us to be\\xa0 more creative and to share that\\ncreativity with each other. [Cheers and Applause].\\n>>DEMIS HASSABIS:\\xa0\\xa0 Over the\\ncoming weeks some of these\\xa0 features will be available to\\nselect creators through VideoFX\\xa0 at labs.google, and the waitlist\\nis open now.\\xa0 Of course, these advances in\\ngenerative video go beyond the\\xa0 beautiful visuals you’ve seen\\ntoday.\\xa0 By teaching future AI models how\\nto solve problems creatively, or\\xa0 in effect simulate the physics\\nof our world, we can build more\\xa0 useful systems that help people\\ncommunicate in new ways, and\\xa0 thereby advance the frontiers of\\nAI.\\xa0 When we first began this journey\\xa0\\nto build AI more than 15 years ago,\\xa0\\xa0 we knew that one day it would change everything.\\xa0 Now that time is here.\\nAnd we continue to be amazed by\\xa0 the progress we see and inspired\\nby the advances still to come,\\xa0 on the path to AGI.\\nThanks, and back to you Sundar. [Applause].\\n>>SUNDAR PICHAI: Thanks, Demis.\\xa0 A huge amount of innovation is\\nhappening at Google DeepMind.\\xa0 it’s amazing how much progress\\nwe have made in a year.\\xa0 Training state-of-the-art models\\nrequires a lot of computing\\xa0 power.\\nIndustry demand for ML compute\\xa0 has grown by a factor of 1\\nmillion in the last six years.\\xa0 And every year, it increases\\ntenfold.\\xa0 Google was built for this.\\nFor 25 years, we’ve invested in\\xa0 world-class technical\\ninfrastructure, from the\\xa0 cutting-edge hardware that\\npowers Search, to our custom\\xa0 tensor processing units that\\npower our AI advances.\\xa0 Gemini was trained and served\\nentirely on our fourth and fifth\\xa0 generation TPUs.\\nAnd other leading AI companies,\\xa0 like Anthropic, have trained\\ntheir models on TPUs as well.\\xa0 Today, we are excited to announce the\\xa0\\nsixth generation of TPUs called Trillium. [Cheers and Applause].\\nTrillium delivers a 4.7x\\xa0 improvement in compute\\nperformance per chip over the\\xa0 previous generation.\\nSo our most efficient and performant TPU to date.\\xa0 We will make trillium available to\\xa0\\nour cloud customers in late 2024.\\xa0 Alongside our TPUs, we are proud to offer\\xa0\\nCPUs and GPUs to support any workload.\\xa0 That includes the new Axion processes we\\xa0\\nannounced last month, our first custom\\xa0\\xa0 on-base CPU with industry-leading\\xa0\\nperformance and energy efficiency.\\xa0 We are also proud to be one of the first\\xa0\\ncloud providers to offer Nvidia\\'s cutting edge\\xa0 Blackwell GPUs, available in\\nearly 2025. [Applause].\\nWe’re fortunate to have a\\xa0 longstanding partnership with\\nNvidia, and are excited to bring\\xa0 Blackwell\\'s capabilities to our customers.\\nChips are a foundational part of\\xa0 our integrated end-to-end\\nsystem, from\\xa0 performance-optimized hardware\\nand open software to flexible\\xa0 consumption models.\\nThis all comes together in our\\xa0 AI Hypercomputer, a\\ngroundbreaking supercomputer\\xa0 architecture.\\nBusinesses and developers are\\xa0 using it to tackle more complex\\nchallenges, with more than twice\\xa0 the efficiency relative to just\\nbuying the raw hardware and\\xa0 chips.\\nOur AI Hypercomputer\\xa0 advancements are made possible\\nin part because of our approach\\xa0 to liquid cooling in our data\\ncenters.\\xa0 We’ve been doing this for nearly\\na decade, long before it became\\xa0 state of the art for the\\nindustry.\\xa0 And today, our total deployed\\nfleet capacity for liquid\\xa0 cooling systems is nearly 1 Giga\\nWatt, and growing.\\xa0 That’s close to 70 times the\\ncapacity of any other fleet. [Applause].\\nUnderlying this is the sheer\\xa0 scale of our network, which\\nconnects our infrastructure\\xa0 globally.\\nOur network spans more than 2\\xa0 million miles of terrestrial and\\nsubsea fiber: Over 10 times the\\xa0 reach of the next leading cloud\\nprovider.\\xa0 We will keep making the\\ninvestments necessary to advance\\xa0 AI innovation and deliver\\nstate-of-the-art capabilities.\\xa0 And one of our greatest areas of\\ninvestment and innovation is in\\xa0 our founding product, Search.\\n25 years ago we created Search\\xa0 to help people make sense of the\\nwaves of information moving\\xa0 online.\\nWith each platform shift, we’ve\\xa0 delivered breakthroughs to help\\nanswer your questions better.\\xa0 On mobile, we unlocked new types\\nof questions and answers, using\\xa0 better context, location\\nawareness, and real-time\\xa0 information.\\nWith advances in natural\\xa0 language understanding and\\ncomputer vision, we enabled new\\xa0 ways to search with your voice,\\nor a hum to find your new\\xa0 favorite song, or an image of\\nthat flower you saw on your\\xa0 walk.\\nAnd now you can even circle to\\xa0 Search those cool new shoes you\\nmight want to buy.\\xa0 Go for it, you can always return\\nthem later!\\xa0 Of course, Search in the Gemini\\nEra will take this to a whole\\xa0 new level.\\nCombining our infrastructure\\xa0 strengths, the latest AI\\ncapabilities, our high bar for\\xa0 information quality, and our\\ndecades of experience connecting\\xa0 you to the richness of the web.\\nThe result is a product that\\xa0 does the work for you.\\nGoogle Search is generative AI\\xa0 at the scale of human curiosity.\\nAnd it’s our most exciting\\xa0 chapter of Search yet.\\nTo tell you more, here’s Liz. [Applause].\\n>>LIZ REID:\\xa0\\xa0 Thanks, Sundar!\\nWith each of these platform\\xa0 shifts, we haven’t just adapted,\\nwe’ve expanded what’s possible\\xa0 with Google Search.\\nAnd now, with generative AI,\\xa0 Search will do more for you than\\nyou ever imagined.\\xa0 So whatever’s on your mind, and\\nwhatever you need to get done,\\xa0 just ask.\\nAnd Google will do the Googling\\xa0 for you.\\nAll the advancements you’ll see\\xa0 today are made possible by a new\\nGemini model, customized for\\xa0 Google Search.\\nWhat really sets this apart is\\xa0 our three unique strengths.\\nFirst, our real-time information\\xa0 with over a trillion facts about\\npeople, places, and things.\\xa0 Second, our unparalleled ranking\\nand quality systems, trusted for\\xa0 decades to get you the very best\\nof the web.\\xa0 And third, the power of Gemini,\\nwhich unlocks new agentive\\xa0 capabilities, right in Search.\\nBy bringing these three things\\xa0 all together, we are able to dramatically expand\\xa0\\nwhat\\'s possible with Google Search, yet again.\\xa0 This is Search in the Gemini\\nera.\\xa0 So let\\'s dig in.\\nYou\\'ve heard today about AI\\xa0 Overviews, and how helpful\\npeople are finding them.\\xa0 With AI Overviews, Google does\\nthe work for you.\\xa0 Instead of piecing together all\\nthe information yourself, you\\xa0 can ask your question, and as you see\\xa0\\nhere, you can get an answer instantly.\\xa0 Complete with a range of\\nperspectives and links to dive\\xa0 deeper.\\nAs Sundar shared, AI Overviews\\xa0 will begin rolling out to\\neveryone in the U.S. starting\\xa0 today, with more countries soon.\\nBy the end of the year, AI\\xa0 Overviews will come to over a\\nbillion people in Google Search.\\xa0 But this is just the first step.\\nWe’re making AI Overviews even\\xa0 more helpful for your most\\ncomplex questions, the type that\\xa0 are really more like ten\\nquestions in one!\\xa0 You can ask your entire\\nquestion, with all its\\xa0 sub-questions, and get an AI\\noverview in just seconds.\\xa0 To make this possible, we’re\\nintroducing multi-step reasoning\\xa0 in Google Search.\\nSo Google can do the researching\\xa0 for you.\\nFor example, let’s say you’ve\\xa0 been trying to get into yoga and\\nPilates.\\xa0 Finding the right studio can\\ntake a lot of research.\\xa0 There are so many factors to\\nconsider!\\xa0 Soon you’ll be able to ask\\nSearch to: Find the best yoga or\\xa0 Pilates studios in Boston.\\nAnd show you details on their\\xa0 intro offers, and walking time\\nfrom Beacon Hill.\\xa0 As you can see here, Google gets\\nto work for you, finding the\\xa0 most relevant information and\\nbringing it together in your AI\\xa0 Overview.\\nYou get some studios with great\\xa0 ratings and their intro offers.\\nYou can see the distance for\\xa0 each, like this one is just a\\nten-minute walk away!\\xa0 Right below, you see where they\\'re\\xa0\\nlocated, laid out visually.\\xa0 And you\\'ve got all this from just a single search!\\nUnder the hood, our custom\\xa0 Gemini model acts as your AI\\nagent, using what we call\\xa0 multi-step reasoning.\\nIt breaks your bigger question\\xa0 down into all its parts, and\\nit figures out which problems it\\xa0\\xa0 needs to solve and in what order.\\nAnd thanks to our real-time info\\xa0 and ranking expertise, it\\nreasons using the\\xa0 highest-quality information out\\nthere.\\xa0 So since you\\'re asking about\\nplaces, it taps into Google\\'s\\xa0 index of information about the\\nreal world, with over 250\\xa0 million places, and updated in real-time.\\nIncluding their ratings,\\xa0 reviews, business hours, and\\nmore.\\xa0 Research that might have taken\\nyou minutes or even hours,\\xa0 Google can now do on your behalf in just seconds.\\nNext, let me show you another\\xa0 way multi-step reasoning in\\nGoogle Search can make your life\\xa0 that much easier.\\nTake planning, for example.\\xa0 Dreaming up trips and meal plans\\ncan be fun, but doing the work\\xa0 of actually figuring it all out,\\nno, thank you.\\xa0 With Gemini in Search, Google\\ndoes the planning with you.\\xa0 Planning is really hard for AI\\nto get right.\\xa0 It\\'s the type of problem that\\ntakes advanced reasoning and\\xa0 logic.\\nAfter all, if you\\'re meal\\xa0 planning, you probably don’t\\nwant mac\\'n cheese for breakfast,\\xa0 lunch and dinner.\\nOkay, my kids might.\\xa0 But say you’re looking for a bit\\nmore variety.\\xa0 Now, you can ask Search to:\\nCreate a three-day meal plan for\\xa0 a group that’s easy to prepare.\\nAnd here you get a plan with a\\xa0 wide range of recipes from\\nacross the web.\\xa0 This one for overnight oats\\nlooks particularly interesting.\\xa0 And you can easily head over to\\nthe Web site to learn how to prepare them.\\xa0 If you want to get more veggies\\nin, you can simply ask Search to\\xa0 swap in a vegetarian dish.\\nAnd just like that, Search\\xa0\\xa0 customizes your meal plan.\\nAnd you can export your meal\\xa0 plan or get the ingredients as a\\nlist, just by tapping here.\\xa0 Looking ahead, you could imagine\\nasking Google to add everything\\xa0 to your preferred shopping cart.\\nThen, we’re really cooking!\\xa0 These planning capabilities mean\\nSearch will be able to help plan\\xa0 everything from meals and trips\\nto parties, dates, workout\\xa0 routines and more.\\nSo you can get all the fun of\\xa0\\xa0 planning without any of the hassle.\\nYou’ve seen how Google Search\\xa0 can help with increasingly\\ncomplex questions and planning.\\xa0 But what about all those times when you\\xa0\\ndon\\'t know exactly what to ask and you\\xa0\\xa0 need some help brain storming?\\nWhen you come to Search for\\xa0 ideas, you’ll get more than an\\nAI-generated answer.\\xa0 You’ll get an entire\\nAI-organized page, custom-built\\xa0 for you and your question.\\nSay you’re heading to Dallas to\\xa0 celebrate your anniversary and\\nyou\\'re looking for the perfect restaurant.\\xa0 What you get here breaks AI out\\nof the box and it brings it to the whole page.\\xa0 Our Gemini model uncovers the\\nmost interesting angles for you\\xa0 to explore and organizations these\\xa0\\nresults into these helpful clusters.\\xa0 Like, you might have never\\nconsidered restaurants with live\\xa0 music.\\nOr ones with historic charm!\\xa0 Our model even uses contextual\\nfactors, like the time of year.\\xa0 So since it’s warm in Dallas,\\nyou can get rooftop patios as an idea.\\xa0 And it pulls everything together\\ninto a dynamic, whole-page\\xa0 experience.\\nYou’ll start to see this new\\xa0 AI-organized search results page\\nwhen you look for inspiration,\\xa0 starting with dining and\\nrecipes, and coming to movies,\\xa0 music, books, hotels, shopping,\\nand more. [Applause].\\xa0 Today, you’ve seen how you can\\nbring any question to Search,\\xa0 and Google takes the work out of\\nsearching.\\xa0 But your questions aren’t\\nlimited to words in a text box,\\xa0 and sometimes, even a picture\\ncan’t tell the whole story.\\xa0 Earlier, Demis showed you our\\nlatest advancements in video\\xa0 understanding.\\xa0 And I\\'m really excited to share that soon\\xa0\\nyou\\'ll be able to ask questions with video,\\xa0\\xa0 right in Google Search.\\nLet me introduce Rose to show\\xa0 you this in a live demo.\\n[Applause]. >>ROSE YAO: Thank you, Liz!\\nI have always wanted a record player,\\xa0\\xa0 and I got this one, and some\\xa0\\nvinyls at a yard sale recently.\\xa0 But, umm, when I go to play a\\nit, this thing keeps sliding off.\\xa0 I have no idea how to fix\\xa0\\nit or where to even start!\\xa0 Before, I would have pieced\\ntogether a bunch of searches to\\xa0 try to figure this out, like,\\nwhat make is this record player?\\xa0 What’s the model?\\nAnd, what is this thing actually\\xa0 called?\\nBut now I can just ask with a\\xa0 video.\\nSo let\\'s try it.\\xa0 Let\\'s do a live demo.\\nI\\'m going to take a video and ask Google,\\xa0\\xa0 why will this not stay in place?\\nAnd in a near instant,\\xa0\\xa0 Google gives me an AI overview.\\nI get some reasons this might be\\xa0 happening, and steps I can take\\nto troubleshoot.\\xa0 So it looks like first, this is called a tone arm.\\nVery helpful.\\xa0 And it looks like it may be unbalanced,\\xa0\\nand there\\'s some really helpful steps here.\\xa0 And I love that because I\\'m new to all this.\\nI can check out this helpful link from Audio\\xa0\\xa0 Technica to learn even more.\\nSo that was pretty quick! [Applause].\\xa0 Let me walk you through what just happened.\\nThanks to a combination of our\\xa0 state-of-the-art speech models,\\nour deep visual understanding,\\xa0 and our custom Gemini model,\\nSearch was able to understand\\xa0 the question I asked out loud\\nand break down the video\\xa0 frame-by-frame.\\nEach frame was fed into Gemini’s\\xa0 long context window that you\\nheard about earlier today.\\xa0 Search could then pinpoint the\\nexact make and model of my\\xa0 record player.\\nAnd make sense of the motion\\xa0 across frames to identify that\\nthe tonearm was drifting.\\xa0 Search fanned out and combed the\\nweb to find relevant insights\\xa0 from articles, forums, videos,\\nand more.\\xa0 And it stitched all of this\\ntogether into my AI Overview.\\xa0 The result was music to my ears!\\nBack to you, Liz.\\xa0 [Applause].\\n>>LIZ REID: Everything you saw\\xa0 today is just a glimpse of how\\nwe\\'re reimagining Google Search\\xa0 in the Gemini era.\\nWe’re taking the very best of\\xa0 what makes Google, Google.\\nAll the reasons why billions of\\xa0 people turn to Google Search,\\nand have relied on us for\\xa0 decades.\\nAnd we’re bringing in the power\\xa0 of Gemini’s agentive\\ncapabilities.\\xa0 So Google will do the searching,\\nthe Researching.\\xa0 The planning.\\nThe brainstorming.\\xa0 And so much more.\\nAll you need to do, is ask.\\xa0 You\\'ll start to see these\\nfeatures rolling out in Search\\xa0 in the coming weeks.\\nOpt in to Search Labs to be\\xa0 among the first to try them out.\\nNow let\\'s take a look at how\\xa0 this all comes together in\\nGoogle Search this year.\\xa0 >> Why is the lever not moving\\nall the way? [Applause]. >>APARNA PAPPU:\\xa0\\xa0 Since last May, we\\'ve been hard at\\xa0\\nwork making Gemini for workspace\\xa0\\xa0 even more helpful for businesses\\xa0\\nand consumers across the world.\\xa0 Tens of thousands of customers\\nhave been using help me write,\\xa0 help me visualize and help me\\norganize since we launched.\\xa0 And now, we\\'re really excited\\nthat the new Gemini powered side\\xa0 panel will be generally\\navailable next month. [Cheers and Applause].\\nOne of our customers is a local\\xa0 favorite right here in\\nCalifornia, Sports Basement.\\xa0 They rolled out Gemini for\\nWorkspace to the organization.\\xa0 And this has helped improve the productivity of\\xa0\\ntheir customer support team by more than 30%.\\xa0 Customers love how Gemini grows\\xa0\\nparticipation in meetings with\\xa0\\xa0 automatic language detection and real-time\\xa0\\ncaptions now expanding to 68 languages. [Applause].\\nWe are really excited about what\\xa0 Gemini 1.5 Pro unlocks for\\nWorkspace and AI Premium\\xa0 customers.\\nLet me start by showing you\\xa0 three new capabilities coming to\\nGmail mobile.\\xa0 This is my Gmail account.\\nOkay.\\xa0 So there\\'s an E-mail up top from my husband.\\nHelp me sort out the roof repair thing, please.\\xa0 Now, we\\'ve been trying to find a\\xa0\\ncontractor to fix our roof, and with\\xa0\\xa0 work travel, I have clearly dropped the ball.\\nIt looks like there\\'s an E-mail thread on this\\xa0\\xa0 with lots of E-mails that I haven\\'t read.\\nAnd luckily for me, I can simply tap the\\xa0\\xa0 summarize option up top and skip\\xa0\\nreading this long back and forth.\\xa0 Now, Gemini pulls up this helpful\\xa0\\nmobile card as an overlay.\\xa0 And this is where I can read a nice summary of\\xa0\\nall the salient information that I need to know.\\xa0 So here I see that we have a\\nquote from Jeff at Green\\xa0 Roofing, and he\\'s ready to start.\\nNow, I know we had other bids\\xa0\\xa0 and I don\\'t remember the details.\\nPreviously, I would have had to do\\xa0\\xa0 a number of searches in G-mail and then remember\\xa0\\nand compare information across different E-mails.\\xa0 Now, I can simply type out my question right here\\xa0\\nin the mobile card and say something like, compare\\xa0\\xa0 my roof repair bids by price and availability.\\nThis new Q&A feature makes it so easy to get\\xa0\\xa0 quick answers on anything in my inbox.\\nFor example, when are my shoes arriving,\\xa0\\xa0 or what time do doors open for the\\xa0\\nKnicks game, without having to first\\xa0\\xa0 search G-mail and open an E-mail and look for the\\xa0\\nspecific information in attachments and so on.\\xa0 Anyway, back to my roof.\\nIt looks like Gemini has found details that I got\\xa0\\xa0 from two other contractors in completely different\\xa0\\nE-mail threads, and I have this really nicely\\xa0\\xa0 organized summary and I can do a quick comparison.\\nSo it seems like Jeff\\'s quote was right\\xa0\\xa0 in the middle and he can start\\xa0\\nimmediately, so Green Roofing it is.\\xa0 I\\'ll open that last E-mail from\\xa0\\nJeff and confirm the project.\\xa0 And look at that.\\nI see some suggested replies from Gemini.\\xa0 Now, what is really, really neat about this\\xa0\\nevolution of smart reply is that it\\'s contextual.\\xa0 Gemini understood the back-and-forth in that\\xa0\\nthread and that Jeff was ready to start.\\xa0 So offers me a few customize\\xa0\\noptions based on that context.\\xa0 So, you know, here I see I have decline\\xa0\\nthe service, suggest a new time.\\xa0 I\\'ll choose proceed and confirm time.\\nI can even see a preview of the\\xa0\\xa0 full reply simply by long pressing.\\nThis looks reasonable, so I\\'ll hit send.\\xa0 These new capabilities in Gemini and G-mail\\xa0\\nwill start rolling out this month to labs users. [Applause].\\nOkay.\\xa0 So one of the really neat things\\nabout WorkSpace apps like\\xa0 G-mail, Drive, Docs, Calendar,\\nis how well they work together,\\xa0 and in our daily lives we often\\nhave information that flows from\\xa0 one app to another.\\nLike, say, adding a calendar entry from G-mail.\\xa0 Or creating reminders from a\\nspreadsheet tracker.\\xa0 But what if Gemini could make\\nthese journeys totally seamless?\\xa0 Perhaps even automate them for\\nyou entirely.\\xa0 Let me show you what I mean with\\na real life example.\\xa0 My sister is a self-employed\\nphotographer, and her in box is\\xa0 full of appointment bookings, receipts,\\xa0\\nclient feedback on photos and so much more.\\xa0 Now, if you\\'re a freelancer or a small\\xa0\\nbusiness, you really want to focus on your\\xa0\\xa0 craft and not on bookkeeping and logistics.\\nSo let\\'s go to her in box and take a look.\\xa0 Lots of unread E-mails.\\nLet\\'s click on the first one.\\xa0 It\\'s got a PDF attachment.\\nFrom a hotel, there\\'s a receipt.\\xa0 And I see a suggestion in the side panel.\\nHelp me organize and track my receipts.\\xa0 Let\\'s click on this prompt.\\nThe side panel now will show\\xa0\\xa0 me more details about what that really means,\\xa0\\nand as you can see, there\\'s two steps here.\\xa0 Step one, create a Drive folder and put this\\xa0\\nreceipt and 37 others it\\'s found into that folder.\\xa0 Makes sense.\\nStep 2,\\xa0\\xa0 extract the relevant information from those\\xa0\\nreceipts in that folder into a new spreadsheet.\\xa0 Now, this sounds useful.\\nWhy not?\\xa0 I also have the option to edit\\xa0\\nthese actions or just hit okay.\\xa0 So let\\'s hit okay.\\nGemini will now complete the two\\xa0 steps described above, and this\\xa0\\nis where it gets even better.\\xa0 Gemini offers the option to automate this\\xa0\\nso that this particular work flow is run on\\xa0\\xa0 all future E-mails, keeping your Drive folder and\\xa0\\nexpense sheet up to date with no effort from you. [Applause].\\nNow, we know that creating\\xa0 complex spread sheets can be\\ndaunting for most people.\\xa0 But with this automation, Gemini\\ndoes the hard work of extracting\\xa0 all the right information from all the files from\\xa0\\nin that folder and generates the sheet for you.\\xa0 So let\\'s take a look.\\nOkay.\\xa0 It\\'s super well organized, and it\\xa0\\neven has a category for expense type.\\xa0 Now, we have this sheet.\\nThings can get even more fun.\\xa0 We can ask Gemini questions.\\nQuestions like, show me where the money is spent.\\xa0 Gemini not only analyzes the data from the\\xa0\\nsheet, but also creates a nice visual to\\xa0\\xa0 help me see the complete breakdown by category.\\nAnd you can imagine how this extends to all sorts\\xa0\\xa0 of use cases in your in box, like travel expenses,\\xa0\\nshopping, remodeling projects, you name it.\\xa0 All of that information in G-mail can be put to\\xa0\\ngood use and help you work, plan and play better.\\xa0 Now, this particular --\\n[Applause].\\xa0 I know!\\xa0 This particular ability to organize your\\xa0\\nattachments in Drive and generate a sheet\\xa0\\xa0 and do data analysis via Q&A will be\\xa0\\nrolling out to Labs users this September.\\xa0 And it\\'s just one of the many automations\\xa0\\nthat we\\'re working on in WorkSpace.\\xa0 Workspace in the Gemini era will\\ncontinue to unlock new ways of\\xa0 getting things done.\\nWe’re building advanced agentive\\xa0 experiences, including\\ncustomizing how you use Gemini.\\xa0 Now, as we look to 2025 and\\nbeyond, we\\'re exploring\\xa0\\xa0 entirely new ways of working with AI.\\nNow, with Gemini, you have an AI-powered\\xa0\\xa0 assistant always at your side.\\nBut what if you could expand how\\xa0 you interact with AI?\\nFor example, when we work with\\xa0 other people, we mention them in comments\\xa0\\nand docs, so we send them E-mail.\\xa0 We have group chats with them, et cetera.\\nAnd it\\'s not just how we collaborate with\\xa0\\xa0 each other, but we each have a\\xa0\\nspecific role to play in the team.\\xa0 And as the team works together, we build a\\xa0\\nset of collective experiences and contexts\\xa0\\xa0 to learn from each other.\\nWe have the combined set of\\xa0 skills to draw from when we need help.\\nSo how could we introduce AI into this mix\\xa0\\xa0 and build on this shared expertise?\\nWell, here’s one way.\\xa0 We are prototyping a virtual\\nGemini powered teammate.\\xa0 This teammate has an identity\\nand a Workspace account, along\\xa0 with a specific role and\\nobjective.\\xa0 Let me bring Tony up to show you what I mean.\\nHey, Tony!\\xa0 >>TONY VINCENT: Hi, Aparna!\\nHey, everyone.\\xa0 Okay.\\nSo let me start by showing you\\xa0 how we set up this virtual\\nteammate.\\xa0 As you can see, the teammate has\\nits very own account.\\xa0 And we can go ahead and give it a name.\\nWe\\'ll do something fun like Chip.\\xa0 Chip’s been given a specific\\nAnd set of descriptions on how to be helpful\\xa0\\xa0 for the team, you can see that here, and some\\xa0\\nof the jobs are to monitor and track projects,\\xa0\\xa0 we\\'ve listed a few out, to organize information\\xa0\\nand provide contexts, and a few more things.\\xa0 Now that we\\'ve configured our virtual teammate,\\xa0\\nlet\\'s go ahead and see Chip in action.\\xa0 To do that I\\'ll switch us\\xa0\\nover here to Google chat.\\xa0 First, when planning for an event like I/O, we\\xa0\\nhave a ton of chat rooms for various purposes.\\xa0 Luckily for me, chip is in all of them.\\nTo quickly catch up, I might ask a question like,\\xa0\\xa0 anyone know if our I/O storyboards are approved?\\nBecause we’ve instructed Chip to\\xa0 track this project, Chip\\nsearches across all the conversations\\xa0\\xa0 and knows to respond with an answer.\\nThere it is.\\xa0 Simple, but very helpful.\\nNow, as the team adds Chip to more\\xa0\\xa0 group chats, more files, more E-mail threads, Chip\\xa0\\nbuilds a collective memory of our work together.\\xa0 Let\\'s look at an example.\\nTo show you I\\'ll switch over to a different room.\\xa0 How about Project Sapphire over here and here we\\xa0\\nare discussing a product release coming up and\\xa0\\xa0 as usual, many pieces are still in flight, so I\\xa0\\ncan go ahead and ask, are we on track for launch?\\xa0 Chip gets to work not only searching\\xa0\\nthrough everything it has access to,\\xa0\\xa0 but also synthesizing what\\'s found and\\xa0\\ncoming back with an up-to-date response.\\xa0 There it is.\\nA clear time line, a nice summary and\\xa0\\xa0 notice even in this first message here, Chip flags\\xa0\\na potential issue the team should be aware of.\\xa0 Because we\\'re in a group space, everyone can\\xa0\\nfollow along, anyone can jump in at any time,\\xa0\\xa0 as you see someone just did.\\nAsking Chip to help create a\\xa0\\xa0 doc to help address the issue.\\nA task like this could take me\\xa0 hours, dozens of hours.\\nChip can get it all done in just a few minutes,\\xa0\\xa0 sending the doc over right when it\\'s ready.\\nAnd so much of this practical helpfulness\\xa0\\xa0 comes from how we\\'ve customized Chip to our team\\'s\\xa0\\nneeds, and how seamlessly this AI is integrated\\xa0\\xa0 directly into where we\\'re already working.\\nBack to you, Aparna. >>APARNA PAPPU: Thank you, Tony!\\nI can imagine a number of\\xa0 different types of virtual\\nteammates configured by\\xa0 businesses to help them do what they need.\\nNow, we have a lot of work to do to figure out how\\xa0\\xa0 to bring these agentive experiences like virtual\\xa0\\nteammates into WorkSpace, including enabling third\\xa0\\xa0 parties to make their very own versions of Chip.\\nWe\\'re excited about where this is headed,\\xa0\\xa0 so stay tuned.\\nAnd as Gemini and its capabilities continue\\xa0\\xa0 to evolve, we\\'re diligently bringing that power\\xa0\\ndirectly into WorkSpace to make all our users more\\xa0\\xa0 productive and creative, both at home and at work.\\nAnd now, over to Sissie to tell you more about\\xa0\\xa0 Gemini app.\\n[Applause].\\xa0 >>SISSIE HSIAO: Our vision for\\nthe Gemini app is to be the most\\xa0 helpful, personal AI assistant\\nby giving you direct access to\\xa0 Google’s latest AI models.\\nGemini can help you learn,\\xa0 create, code, and anything else\\nyou can imagine.\\xa0 And over the past year, Gemini\\nhas put Google’s AI in the hands\\xa0 of millions of people, with\\nexperiences designed for your\\xa0 phone and the web.\\nWe also launched Gemini\\xa0 Advanced, our premium\\nsubscription for access to the\\xa0 latest AI innovations from\\nGoogle.\\xa0 Today, we’ll show you how Gemini\\nis delivering our most\\xa0 intelligent AI experience.\\nLet’s start with the Gemini app,\\xa0 which is redefining how we\\ninteract with AI.\\xa0 It’s natively multimodal, so you\\ncan use text, voice or your\\xa0 phone’s camera to express\\nyourself naturally.\\xa0 And this summer, you can have an\\nin-depth conversation with\\xa0 Gemini using your voice.\\nWe’re calling this new\\xa0 experience \"Live\".\\nUsing Google’s latest speech\\xa0 models, Gemini can better\\nunderstand you and answer\\xa0 naturally.\\nYou can even interrupt while\\xa0 Gemini is responding, and it\\nwill adapt to your speech\\xa0 patterns.\\nAnd this is just the beginning.\\xa0 We\\'re excited to bring the speed games\\xa0\\nand video understanding capabilities\\xa0\\xa0 from Project Astra to the Gemini app.\\nWhen you go live, you\\'ll be able to\\xa0\\xa0 open your camera so Gemini can see what you see\\xa0\\nand respond to your surroundings in real-time.\\xa0 Now, the way I use Gemini\\xa0\\nisn\\'t the way you use Gemini.\\xa0 So we\\'re rolling out a new feature that\\xa0\\nlets you customize it for your own needs.\\xa0 And create personal experts on any topic you want.\\nWe\\'re calling these \"Gems.\" [Applause].\\xa0 They\\'re really simple to set up.\\nJust tap to create a gem, write your instructions\\xa0\\xa0 once, and come back whenever you need it.\\nFor example, here\\'s a gem that I created\\xa0\\xa0 that acts as a personal writing coach.\\nIt specializes in short stories with\\xa0\\xa0 mysterious twists, and it even builds\\xa0\\non the story drafts in my Google drive.\\xa0 I call it the cliff hanger curator.\\nNow, gems are a great time saver when\\xa0\\xa0 you have specific ways that you want to\\xa0\\ninteract with Gemini again and again.\\xa0 Gems will roll out in the coming\\nmonths, and our trusted testers\\xa0 are already finding so many\\ncreative ways to put them to\\xa0 use.\\nThey can act as your yoga\\xa0 bestie, your personal sous chef,\\na brainy calculus tutor, a peer\\xa0 reviewer for your code, and so\\nmuch more.\\xa0 Next, I’ll show you how Gemini\\nis taking a step closer to being\\xa0 a true AI assistant by planning\\nand taking action for you.\\xa0 We all know chatbots can give\\nyou ideas for your next\\xa0 vacation.\\nBut there’s a lot more that goes\\xa0 into planning a great trip.\\nIt requires reasoning that\\xa0 considers space-time logistics,\\nand the intelligence to\\xa0 prioritize and make decisions.\\nThat reasoning and intelligence\\xa0 all comes together in the new\\ntrip planning experience in\\xa0 Gemini Advanced.\\nNow, it all starts with a prompt.\\xa0 Okay.\\nSo here we go.\\xa0 We’re going to Miami.\\nMy son loves art, my husband\\xa0 loves seafood, and our flight\\nand hotel details are already in\\xa0 my Gmail inbox.\\nNow there’s a lot going on in\\xa0 that prompt.\\nEveryone has their own things\\xa0 that they want to do.\\nTo make sense of those\\xa0 variables, Gemini starts by\\ngathering all kinds of\\xa0 information from Search, and\\nhelpful extensions like Maps and\\xa0 G-mail.\\nIt uses that data to create a\\xa0 dynamic graph of possible travel\\noptions, taking into account all\\xa0 my priorities and constraints.\\nThe end result is a personalized\\xa0 vacation plan, presented in\\nGemini’s new dynamic UI.\\xa0 Now, based on my flight\\ninformation, Gemini knows that I\\xa0 need a two and a half day\\nitinerary.\\xa0 And you can see how Gemini uses\\nspatial data to make decisions.\\xa0 Our flight lands in the late\\nafternoon, so Gemini skips a big\\xa0 activity that day, and finds a\\nhighly rated seafood restaurant\\xa0 close to our hotel.\\nNow, on Sunday, we have a jam-packed day.\\xa0 I like these recommendations,\\nbut my family likes to sleep in.\\xa0 So I tap to change the start time,\\xa0\\nand just like that, Gemini adjusted my\\xa0\\xa0 itinerary for the rest of the trip.\\nIt moved our walking tour to the\\xa0 next day and added lunch options\\nnear the street art museum to\\xa0 make the most of our Sunday\\nafternoon.\\xa0 This looks great!\\nIt would have taken me hours of\\xa0 work, checking multiple sources,\\nfiguring out schedules, and\\xa0 Gemini did this in a fraction of\\nthe time.\\xa0 This new trip-planning\\nexperience will be rolling out\\xa0 to Gemini Advanced this summer,\\njust in time to help you plan your\\xa0\\xa0 own Labor Day weekend.\\n[Applause].\\xa0 All right.\\nWe saved the best for last.\\xa0 You heard Sundar say earlier\\nthat starting today, Gemini\\xa0 Advanced subscribers get access\\nto Gemini 1.5 Pro, with one\\xa0 million tokens.\\nThat is the longest context\\xa0 window of any chatbot in the\\nworld. [Cheers and Applause].\\nIt unlocks incredible new\\xa0 potential in AI, so you can\\ntackle complex problems that\\xa0 were previously unimaginable.\\nYou can upload a PDF up to 1,500\\xa0 pages long, or multiple files to\\nget insights across a project.\\xa0 And soon, you can upload as much as 30,000\\xa0\\nlines of code or even an hour-long video.\\xa0 Gemini Advanced is the only\\nchatbot that lets you process\\xa0 this amount of information.\\nNow, just imagine how useful\\xa0 this will be for students.\\nLet’s say you’ve spent months on\\xa0 your thesis, and you could\\xa0\\nreally use a fresh perspective.\\xa0 You can upload your entire\\nthesis, your sources, notes,\\xa0 your research, and soon interview,\\xa0\\naudio recordings and videos, too.\\xa0 so Gemini has all this context\\nto give you actionable advice.\\xa0 It can dissect your main points,\\nidentify improvements, and even\\xa0 role play as your professor.\\nSo you can feel confident in\\xa0 your work.\\nAnd check out what Gemini\\xa0 Advanced can do with your\\nspreadsheets, with the new data\\xa0 analysis feature launching in\\nthe coming weeks.\\xa0 Maybe you have a side hustle\\nselling handcrafted products.\\xa0 But you’re a better artist than\\naccountant, and it\\'s really hard to understand\\xa0\\xa0 which products are worth your time.\\nSimply upload all of your\\xa0 spreadsheets and ask Gemini to\\nvisualize your earnings and help\\xa0 you understand your profit.\\nGemini goes to work calculating\\xa0 your returns and pulling its\\nanalysis together into a single\\xa0 chart, so you can easily\\nunderstand which products are\\xa0 really paying off.\\nNow, behind the scenes, Gemini writes\\xa0\\xa0 custom Python code to crunch these numbers.\\nAnd of course, your files are\\xa0 not used to train our models.\\nOh, and just one more thing.\\xa0 Later this year, we\\'ll be doubling the\\xa0\\nlong context window to 2 million tokens. [Cheers and Applause].\\nWe absolutely can\\'t wait for\\xa0\\xa0 you to try all of this for yourself.\\nGemini is continuing to evolve\\xa0 and improve at a breakthrough\\npace.\\xa0 We’re making Gemini more\\nmultimodal, more agentive, and\\xa0 more intelligent, with the\\ncapacity to process the most\\xa0 information of any chatbot in\\nthe world.\\xa0 And as you heard earlier, we\\'re\\nalso expanding Gemini Advanced\\xa0 to over 35 supported languages,\\navailable today. [Applause].\\nBut, of course, what makes\\xa0 Gemini so compelling is how easy\\xa0 it is to do just about anything\\nyou want, with a simple prompt.\\xa0 Let\\'s take a look.\\n>> Enter prompt here.\\xa0 Okay.\\nCan\\'t be that hard.\\xa0 How about generate an image of a\\ncat playing guitar?\\xa0 Is that how it works?\\nAm I doing AI?\\xa0 Yeah.\\nJust does whatever you type\\xa0 What are last minute gift ideas\\nyou can make with arts and\\xa0 crafts?\\nPlan a workout routine to get\\xa0 bigger calves.\\nHelp me think of titles to my\\xa0 tell-all memoir.\\nWhat\\'s something smart I can say\\xa0 about Renoir?\\nGenerate another image of a cat\\xa0 playing guitar.\\nIf a girl calls me a snack, how\\xa0 do I reply?\\nYeah, that\\'s how it works.\\xa0 you\\'re doing AI.\\nMake this email sound more\\xa0 professional before I hit send.\\nWhat\\'s a good excuse to cancel\\xa0 dinner with my friends?\\nWe\\'re literally sitting right\\xa0 here.\\nThere\\'s no wrong way to prompt.\\xa0 Yeah, you\\'re doing AI.\\nThere\\'s no wrong way to prompt.\\xa0 It does whatever you type.\\nJust prompt your prompt in the\\xa0 prompt bar.\\nOr just generate an image of a\\xa0 cat playing guitar.\\nYou know it can do other stuff,\\xa0 right?\\n[Applause].\\xa0 >>SAMEER SAMAT: Hi, everyone.\\nIt’s great to be back at Google\\xa0 I/O.\\nToday, you’ve seen how AI is\\xa0 transforming our products across\\nGemini, Search, Workspace and\\xa0 more.\\nWe\\'re bringing all of these\\xa0 innovations right onto your\\nAndroid phone.\\xa0 And we\\'re going even further, to\\nmake Android the best place to\\xa0 experience Google AI.\\nThis new era of AI is a profound\\xa0 opportunity to make smartphones\\ntruly smart.\\xa0 Our phones have come a long way\\nin a short time, but if you\\xa0 think about it, it’s been years\\nsince the user experience has\\xa0 fundamentally transformed.\\nThis is a once-in-a-generation\\xa0 moment to reinvent what phones\\ncan do.\\xa0 So we’ve embarked on a\\nmulti-year journey to reimagine\\xa0 Android, with AI at the core.\\nAnd it starts with three\\xa0 breakthroughs you’ll see this\\nyear.\\xa0 First, we\\'re putting AI-powered\\nsearch right at your fingertips,\\xa0 creating entirely new ways to\\nget the answers you need.\\xa0 Second, Gemini is becoming your\\nnew AI assistant on Android,\\xa0 there to help you any time.\\nAnd third, we’re harnessing\\xa0 on-device AI to unlock new\\nexperiences that work as fast as\\xa0 you do, while keeping your\\nsensitive data private.\\xa0 Let\\'s start with AI-powered\\nsearch.\\xa0 Earlier this year, we took an\\nimportant first step at Samsung\\xa0 Unpacked, by introducing Circle\\nto Search.\\xa0 It brings the best of Search\\ndirectly into the user\\xa0 experience.\\nSo you can go deeper on anything\\xa0 you see on your phone, without\\nswitching apps.\\xa0 Fashionistas are finding the\\nperfect shoes, home chefs are\\xa0 discovering new ingredients, and\\nwith our latest update, it’s\\xa0 never been easier to translate\\nwhatever’s on your screen, like\\xa0 a social post in another\\nlanguage.\\xa0 And there are even more ways\\nCircle to Search can help.\\xa0 One thing we’ve heard from\\nstudents is that they are doing\\xa0 more of their schoolwork\\ndirectly on their phones and\\xa0 tablets.\\nSo, we thought: Could Circle to\\xa0 Search be your perfect study\\nbuddy?\\xa0 Let’s say my son needs help with\\na tricky physics word problem,\\xa0 like this one.\\nMy first thought is, oh boy,\\xa0 it’s been a while since I’ve\\nthought about kinematics.\\xa0 If he’s stumped on this\\nquestion, instead of putting me\\xa0 on the spot, he can circle the\\nexact part he’s stuck on and get\\xa0 step-by-step instructions.\\nRight where he’s already doing\\xa0 the work.\\nAh, of course, final velocity\\xa0 equals initial velocity plus\\nacceleration times elapsed time.\\xa0 Right.\\nI was just about to say that.\\xa0 Seriously, though, I love that\\nit shows how to solve the\\xa0 problem, not just the answer.\\nThis new capability is available\\xa0 today!\\nAnd later this year, Circle to\\xa0 Search will be able to tackle\\nmore complex problems involving\\xa0 symbolic formulas, diagrams,\\ngraphs and more.\\xa0 Circle to Search is only on\\nAndroid.\\xa0 It’s available on more than 100\\nmillion devices today, and we’re\\xa0 on track to double that by the\\nend of the year. [Cheers and Applause].\\nYou’ve already heard from Sissie\\xa0 about the incredible updates\\ncoming to the Gemini app.\\xa0 On Android, Gemini is so much\\nmore.\\xa0 It’s becoming a foundational\\npart of the Android experience.\\xa0 Here’s Dave to share more.\\n[Applause].\\xa0 >>DAVE BURKE: Hey, everyone.\\nA couple months ago we launched\\xa0 Gemini on Android.\\nLike Circle to Search, Gemini\\xa0 works at the system level.\\nSo instead of going to a\\xa0 separate app, I can bring Gemini\\nright to what I’m doing.\\xa0 Now, we\\'re making Gemini\\ncontext aware, so it can\\xa0 anticipate what you\\'re trying to\\ndo and provide more helpful\\xa0\\xa0 situations in the moment.\\nIn other words, to be a more\\xa0 helpful assistant.\\nSo let me show you\\xa0\\xa0 how this\\nworks.\\xa0 And I\\'ve got my shiny new Pixel\\n8a here to help me. [Applause].\\xa0 So my friend Pete is asking me\\nif I want to play pickleball\\xa0 this weekend.\\nAnd I know how to play tennis, sort of.\\xa0 I have to say that for the demo.\\nBut I\\'m new to this pickleball thing,\\xa0\\xa0 so I\\'m to reply and try to be funny and\\xa0\\nsay is that like tennis but with pickles?\\xa0 This would be actually a lot funnier with a meme,\\xa0\\nso let me bring up Gemini to help with that,\\xa0\\xa0 and I\\'ll say create image of tennis with pickles.\\nNow, one new thing you\\'ll notice\\xa0 is that the Gemini window hovers\\nin place above the app so that I\\xa0 stay in the flow.\\nOkay.\\xa0 So I generated some pretty good images.\\nWhat\\'s nice is I can drag and drop any of\\xa0\\xa0 these directly into the images below.\\nSo cool, let me send that. [Applause].\\nAll right.\\xa0 So Pete\\'s typing, and he says -- he\\'s\\xa0\\nsending me a video on how to play pickleball.\\xa0 All right.\\nThanks, Pete.\\xa0 Let\\'s tap on that.\\nAnd that launches YouTube but, you know, I only\\xa0\\xa0 have one or two burning questions about the game.\\nI could bring up Gemini to help with that,\\xa0\\xa0 and because it\\'s context-aware, Gemini knows I\\'m\\xa0\\nlooking at a video, so it proactively shows me\\xa0\\xa0 an ask this video chip.\\nSo let me tap on that.\\xa0 And now, I can ask specific\\xa0\\nquestions about the video.\\xa0 So, for example, what is the 2 bounce rule?\\nBecause that\\'s something that I\\'ve heard about but\\xa0\\xa0 don\\'t quite understand in the game.\\nBy the way, this uses signals like\\xa0\\xa0 YouTube\\'s captions, which means you\\xa0\\ncan use it on billions of videos.\\xa0 So give it a moment, and, there.\\nI get a nice,succinct answer.\\xa0 The ball must bounce once on each\\xa0\\nside of the court after a serve.\\xa0 Okay.\\nCool.\\xa0 Let me go back to messages and\\xa0\\nPete\\'s followed up, and he says,\\xa0\\xa0 you\\'re an engineer, so here\\'s the\\xa0\\nofficial rule book for pickleball.\\xa0 Thanks, Pete.\\nPete is very helpful, by the way.\\xa0 Okay.\\nSo we tap on that.\\xa0 It launches a PDF, now, that\\'s an 84-page PDF.\\nI don\\'t know how much time Pete thinks I have.\\xa0 Anyway, us engineers, as you all know,\\xa0\\nlike to work smarter, not harder,\\xa0\\xa0 so instead of trolling through this entire\\xa0\\ndocument, I can pull up Gemini to help.\\xa0 And again, Gemini anticipates what I need,\\xa0\\nand offers me an ask this PDF option.\\xa0 So if I tap on that, Gemini now ingests all\\xa0\\nof the rules to become a pickleball expert,\\xa0\\xa0 and that means I can ask very esoteric questions,\\xa0\\nlike, for example, are spin serves allowed?\\xa0 And let\\'s hit that, because I\\'ve\\xa0\\nheard that rule may be changing.\\xa0 Now, because I\\'m a Gemini advanced user, this\\xa0\\nworks on any PDF and takes full advantage\\xa0\\xa0 of the long context window and there\\'s\\xa0\\njust lots of times where that\\'s useful.\\xa0 For example, let\\'s say you\\'re looking for\\xa0\\na quick answer in an appliance user manual.\\xa0 And there you have it.\\nIt turns out, no, spin serves are not allowed.\\xa0 So Gemini not only gives me a clear answer to my\\xa0\\nquestion, it also shows me exactly where in the\\xa0\\xa0 PDF to learn more.\\nAwesome.\\xa0 Okay.\\nSo that’s a few of the ways\\xa0 that we\\'re enhancing Gemini to be more\\xa0\\ncontext aware and helpful in the moment.\\xa0 And what you\\'ve seen here are the first of\\xa0\\nreally many new ways that Gemini will unlock\\xa0\\xa0 new experiences at the system level,\\xa0\\nand they\\'re only available on Android.\\xa0 You’ll see these, and more,\\ncoming to hundreds of millions of\\xa0\\xa0 devices over the next couple of months.\\nNow, building Google AI directly\\xa0 into the OS elevates the entire\\nsmartphone experience.\\xa0 Android is the first mobile\\noperating system to include a\\xa0 built-in, on-device foundation\\nmodel.\\xa0 This lets us bring Gemini\\ngoodness from the data center\\xa0 right into your pocket.\\nSo the experience is faster,\\xa0\\xa0 while also protecting your privacy.\\nStarting with Pixel later this\\xa0 year, we’ll be expanding what’s\\npossible with our latest model,\\xa0 Gemini Nano with Multimodality.\\nThis means your phone can\\xa0 understand the world the way you\\nunderstand it.\\xa0 So not just through text input, but also\\xa0\\nthrough sights, sounds, and spoken language.\\xa0 Let me give you an example.\\n2.2 billion people experience\\xa0 blindness or low vision.\\nSo several years ago, we\\xa0 developed TalkBack, an\\naccessibility feature that helps\\xa0 people navigate their phone\\nthrough touch and spoken feedback.\\xa0 Helping with images is\\nespecially important.\\xa0 In fact, my colleague Karo, who\\nuses TalkBack, will typically\\xa0 come across 90 unlabeled images\\nper day.\\xa0 Thankfully, TalkBack makes them\\naccessible, and now we’re taking\\xa0 that to the next level with the\\nmultimodal capabilities of\\xa0 Gemini Nano.\\nSo when someone sends Karo a\\xa0 photo, she’ll get a richer and\\nclearer description of what’s\\xa0 happening.\\nOr, let’s say Karo is shopping\\xa0 online for an outfit.\\nNow she can get a crystal clear\\xa0 description of the style and\\ncut to find the perfect look.\\xa0 Running Gemini Nano on-device\\nhelps minimize latency, and the\\xa0 model even works when there\\'s\\xa0\\nno network connection.\\xa0 These improvements to TalkBack\\nare coming later this year.\\xa0 Let me show you another example\\nof what on-device AI can unlock.\\xa0 People lost more than one\\ntrillion dollars to fraud last\\xa0 year.\\nAnd as scams continue to evolve\\xa0 across texts, phone calls, and\\neven videos, Android can help\\xa0 protect you from the bad guys,\\nno matter how they try to reach\\xa0 you.\\nSo let’s say I get rudely\\xa0 interrupted by an unknown caller\\nright in the middle of my\\xa0 presentation.\\n[Phone ringing].\\xa0 >> Hello!\\n>> Hi.\\xa0 I\\'m calling from Save More Bank\\nSecurity Department.\\xa0 Am I speaking to Dave?\\n>>DAVE BURKE: Yes, this is Dave.\\xa0 I’m kinda in the middle of\\nsomething.\\xa0 >> We\\'ve detected some\\nsuspicious activity on your\\xa0 account.\\nIt appears someone is trying to\\xa0 make unauthorized charges.\\n>>DAVE BURKE: Oh, yeah?\\xa0 What kind of charges?\\n>> I can\\'t give you specifics\\xa0 over the phone, but to protect\\nyour account, I’m going to help\\xa0 you transfer your money to a\\nsecure account we’ve set up for\\xa0 you.\\n[Laughter].\\xa0 >>DAVE BURKE: And look at this,\\xa0 my phone gives me a warning that\\nthis call might be a scam! [Applause].\\nGemini Nano alerts me the second\\xa0 it detects suspicious activity,\\nlike a bank asking me to move my\\xa0 money to keep it safe.\\nAnd everything happens right on\\xa0 my phone, so the audio\\nprocessing stays completely\\xa0 private to me and on my device.\\nWe’re currently testing this\\xa0 feature, and we’ll have more\\nupdates to share later in the\\xa0 summer.\\nAnd we’re really just scratching\\xa0 the surface on the kinds of\\nfast, private experiences that\\xa0 on-device AI unlocks.\\nLater this year, Gemini will be\\xa0 able to more deeply understand\\nthe content of your screen,\\xa0 without any information leaving\\nyour phone, thanks to the\\xa0 on-device model.\\nSo, remember that pickleball\\xa0 example earlier?\\nGemini on Android will be able\\xa0 to automatically understand the\\nconversation and provide\\xa0 relevant suggestions, like where\\nto find pickleball clubs near\\xa0 me.\\xa0 And this is a powerful concept that will\\xa0\\nwork across many apps on your phone.\\xa0 In fact, later today at the\\ndeveloper keynote, you’ll hear\\xa0 about how we’re empowering our\\ndeveloper community with our\\xa0 latest AI models and tools like\\nGemini Nano and Gemini in\\xa0 Android Studio.\\nAlso, stay tuned tomorrow for\\xa0 our upcoming Android 15 updates,\\nwhich we can’t wait to share.\\xa0 As we said at the outset, we’re\\nreimagining Android with Gemini\\xa0 at the core.\\nFrom your favorite apps, to the\\xa0 OS itself, we’re bringing the\\npower of AI to every aspect of\\xa0 the smartphone experience.\\nAnd with that, let me hand over\\xa0 to Josh to share more on our\\nlatest news for developers.\\xa0 Thank you.\\n[Applause].\\xa0 >>JOSH WOODWARD: It’s amazing to\\nsee Gemini Nano do all of that\\xa0 directly on Android.\\nThat was our plan all along, to\\xa0 create a natively multimodal\\nGemini in a range of sizes so\\xa0 you all, as developers, can choose\\xa0\\nthe one that works best for you.\\xa0 Throughout the morning, you’ve\\nheard a lot about our Gemini 1.5\\xa0 series, and is I want to talk about\\xa0\\nthe two models you can access today.\\xa0 1.5 Pro, which is getting a\\xa0 series of quality improvements\\nthat go out, right about now,\\xa0\\xa0 and the brand new 1.5 Flash.\\nBoth are available globally in\\xa0 over 200 countries and\\nterritories. [Cheers and Applause].\\nYou can go over to AI Studio\\xa0\\xa0 or Vertex AI if you\\'re a Google cloud\\xa0\\ncustomer and you can give them a try.\\xa0 Now, both models are also\\nnatively multimodal.\\xa0 That means you can interleave\\ntext, images, audio, video as\\xa0 inputs, and pack that massive 1\\nmillion token context window.\\xa0 And if you go to ai.google.dev\\ntoday, you can sign up to try\\xa0 the 2 million token context\\nwindow for 1.5 Pro.\\xa0 We\\'re also adding a bunch of new developer\\xa0\\nfeatures, starting with video frame extraction.\\xa0 That\\'s going to be in the Gemini\\xa0\\nAPI, parallel function calling,\\xa0\\xa0 so you can return more than one function call\\xa0\\nat a time, and my favorite, context caching, so\\xa0\\xa0 you can send all of your files to the model once\\xa0\\nand not have to re-send them over and over again.\\xa0 That should make the long\\xa0\\ncontext even more useful,\\xa0\\xa0 and more affordable.\\nIt ships next month. [Applause].\\nNow, we\\'re using Google\\'s\\xa0 infrastructure to serve these\\xa0 models, so developers like all\\xa0\\nof you can get great prices.\\xa0 1.5 Pro is $7 per 1 million\\ntokens, and I\\'m excited to share\\xa0 that for prompts up to 128K, it\\nwill be 50% less, for $3.50.\\xa0 And 1.5 flash will start at .35\\ncents for 1 million tokens. [Cheers and Applause].\\xa0 Now, one thing you might be wondering is\\xa0\\nwhich model is best for your use case?\\xa0 Here’s how I think about it.\\nWe use 1.5 Pro for complex tasks, where you\\xa0\\xa0 really want the highest quality response, and it\\'s\\xa0\\nokay if it takes a little bit longer to come back.\\xa0 We\\'re using 1.5 Flash for quick tasks, where\\xa0\\nthe speed of the model is what matters the most.\\xa0 And as a developer, you can go try them both\\xa0\\nout today and see what works best for you.\\xa0 Now, I\\'m going to show you how it works here in\\xa0\\nAI Studio, the fastest way to build with Gemini.\\xa0 And we\\'ll pull it up here, and\\xa0\\nyou can see this is AI studio.\\xa0 It\\'s free to use.\\nYou don\\'t have to configure anything to get going.\\xa0 You just go to AI studio.Google.com, log in with\\xa0\\nyour Google account, and you can just pick the\\xa0\\xa0 model here on the right that works best for you.\\nSo one of the ways we\\'ve been using 1.5\\xa0\\xa0 Flash is to actually learn from customer\\xa0\\nfeedback about some of our labs products.\\xa0 Flash makes this possible with its low latency.\\nSo what we did here is we just took a bunch of\\xa0\\xa0 different feedback from our customer forums.\\nYou can put it in to Flash, load up\\xa0\\xa0 a prompt, and hit run.\\nNow, in the background,\\xa0\\xa0 what it\\'s going to do is it\\'s going to go through\\xa0\\nthat 93,000 token pile of information and you\\xa0\\xa0 can see here start streaming it back.\\nNow, this is really helpful because\\xa0\\xa0 it pulls out the themes for us.\\nIt gives us all the right places\\xa0\\xa0 where we can start to look.\\nWe can see this is from some of the\\xa0\\xa0 benefits from Notebook LM, like we showed earlier.\\nNow, what\\'s great about this is that you can take\\xa0\\xa0 something like this in AI Studio, prototyped\\xa0\\nhere in ten seconds, and with one click in\\xa0\\xa0 the upper left, get an API key, or over here in\\xa0\\nthe upper right, just tap get code, and you\\'ve\\xa0\\xa0 got all the model configurations, the safety\\xa0\\nsettings, ready to go, straight into your IDE.\\xa0 Now, over time, if you find that you\\xa0\\nneed more enterprise-grade features\\xa0\\xa0 you can use the same Gemini 1.5 models and\\xa0\\nthe same configurations right in Vertex AI.\\xa0 That way, you can scale up with Google\\xa0\\nCloud as your enterprise needs grow.\\xa0 So that\\'s our newly updated Gemini 1.5 Pro and the\\xa0\\nnew 1.5 Flash, both of which are available today\\xa0\\xa0 globally, and you\\'ll hear a lot more about\\xa0\\nthem in the developer keynote later today. [Applause].\\xa0 Now, let\\'s shift gears and talk\\xa0\\nabout Gemma, our family of open\\xa0 models, which are crucial for\\ndriving AI innovation and\\xa0 responsibility.\\nGemma is being built from the\\xa0\\xa0 same research and technology as Gemini.\\nIt offers top performance and comes in\\xa0\\xa0 light weight 7b and 2b sizes.\\nSince it launched less than\\xa0 three months ago, it’s been\\ndownloaded millions of times\\xa0 across all the major model hubs.\\nDevelopers and researchers have\\xa0 been using it and customizing the base Gemma\\xa0\\nmodel and using some of our pre-trained variants,\\xa0\\xa0 like RecurrentGemma, and CodeGemma,\\xa0\\nand today\\'s newest member, PaliGemma,\\xa0\\xa0 our first vision-language model,\\xa0\\nand it\\'s available right now. [Applause].\\nIt\\'s optimized for\\xa0\\xa0 a range of image captioning, visual Q&A and\\xa0\\nother image labeling tasks, so go give it a try.\\xa0 I\\'m also excited to announce\\xa0\\nthat we have Gemma 2 coming.\\xa0 It\\'s the next generation of Gemma,\\xa0\\nand it will be available in June.\\xa0 One of the top requests we\\'ve heard from\\xa0\\ndevelopers is for a bigger Gemma model,\\xa0\\xa0 but it\\'s still going to fit in the\\xa0\\nsize that\\'s easy for all of you to use.\\xa0 So in a few weeks, we\\'ll\\xa0\\nbe adding a new 27 billion\\xa0\\xa0 parameter model to Gemma 2, and\\xa0\\nhere\\'s what\\'s great about it.\\xa0 This size is optimized by Nvidia to run\\xa0\\non next-gen GPUs and can run efficiently\\xa0\\xa0 on a single TPU host in Vertex AI.\\nSo this quality to size ratio is\\xa0\\xa0 amazing because it will outperform\\xa0\\nmodels more than twice its size.\\xa0 We can\\'t wait to see what\\xa0\\nyou\\'re going to build with it. [Applause].\\xa0 To wrap up, I want to share this inspiring\\xa0\\nstory from India, where developers have been\\xa0\\xa0 using Gemma and its unique tokenization to\\xa0\\ncreate Navrasa, a set of instruction-tuned\\xa0\\xa0 models to expand access to 15 Indic languages.\\nThis builds on our efforts to make information\\xa0\\xa0 accessible in more than 7,000\\xa0\\nlanguages and the world.\\xa0 Take a look.\\n>>AASHI:\\xa0\\xa0 Language is an\\ninteresting problem to solve,\\xa0 actually, and given India has a\\nhuge variety of languages and it\\xa0 changes every five kilometers.\\n>>HARSH: When technology is\\xa0 developed for a particular\\nculture, it won\\'t be able to\\xa0 solve and understand the nuances\\nof a country like India.\\xa0 One of Gemma’s features is an\\nincredibly powerful tokenizer\\xa0 which enables the model to use\\nhundreds of thousands of words,\\xa0 symbols, and characters across\\nso many alphabets and language\\xa0 systems.\\xa0 This large vocabulary is\\ncritical to adapting Gemma to\\xa0 power projects like Navarasa.\\n>>RAMSRI: Navarasa is a model\\xa0 that’s trained for Indic\\nlanguages.\\xa0 It\\'s a fine-tuned model based on\\nGoogle’s Gemma.\\xa0 We built Navarasa to make large\\nlanguage models culturally\\xa0 rooted where people can talk in\\ntheir native language and get\\xa0 the responses in their native\\nlanguage.\\xa0 Our biggest dream is to build a\\nmodel to include everyone from\\xa0 all corners of India.\\n>>GAURAV: We need a technology\\xa0 that will harness AI so that\\neveryone can use it and no one\\xa0 is left behind.\\n>>HARSH: Today the language that\\xa0 you speak in could be the tool\\nand the technology that you use\\xa0 for solving your real-world\\nproblems.\\xa0 And that\\'s the power of\\ngenerative AI that we want to\\xa0 bring to every corner of India\\nand the entire world. [Applause]. [Cheers and Applause].\\n>>JAMES MANYIKA: Listening to\\xa0 everything that’s been announced\\ntoday, it’s clear that AI is\\xa0 already helping people, from\\ntheir everyday tasks to their\\xa0 most ambitious, productive, and\\nimaginative endeavors.\\xa0 Our AI innovations, like\\nmultimodality, long context and\\xa0 agents, are at the cutting-edge\\nof what this technology can do,\\xa0 taking to a whole new level its\\ncapacity to help people.\\xa0 Yet, as with any emerging\\ntechnology, there are still\\xa0 risks and new questions that\\nwill arise as AI advances and\\xa0 its uses evolve.\\nIn navigating these\\xa0 complexities, we are guided by\\nour AI Principles, and we’re\\xa0 learning from our users,\\npartners, and our own research.\\xa0 To us, building AI responsibly\\nmeans both addressing the risks\\xa0 and maximizing the benefits for\\npeople and society.\\xa0 Let me begin with what we’re\\ndoing to address risks.\\xa0 Here, I want to focus on how we\\nare improving our models and\\xa0 protecting against their misuse.\\nBeyond what Demis shared\\xa0 earlier, we are improving our\\nmodels with an industry-standard\\xa0 practice called red-teaming, in\\nwhich we test our own models and try\\xa0\\xa0 to break them to identify weaknesses.\\nAdding to this work, we’re\\xa0 developing a cutting-edge\\ntechnique we call AI-assisted\\xa0 red teaming.\\nThis draws on Google DeepMind\\'s\\xa0 gaming breakthroughs like\\nAlphaGo, where we train AI\\xa0 agents to compete against each\\nother and improve and expand the\\xa0 scope of their red teaming\\ncapabilities.\\xa0 We are developing AI models with\\nthese capabilities to help\\xa0 address adversarial prompting\\nand limit problematic outputs.\\xa0 We’re also improving our models\\nwith feedback from two important\\xa0 groups: Thousands of internal\\nsafety experts with a range of\\xa0\\xa0 disciplines, and a range of independent\\xa0\\nexperts from academia to civil society.\\xa0 Both groups help us identify\\nemerging risks, from\\xa0 cybersecurity threats to\\npotentially dangerous\\xa0 capabilities in areas like\\nChem-Bio.\\xa0 Combining human insight with our\\nsafety testing methods will help\\xa0 make our models and products\\nmore accurate, reliable and safer.\\xa0 This is particularly important\\nas technical advances like better\\xa0\\xa0 intonation make interactions with\\xa0\\nAI feel and sound more human-like.\\xa0 We\\'re doing a lot of research in this area,\\xa0\\nincluding the potential for harm and misuse.\\xa0 We\\'re also developing new tools to\\xa0\\nhelp prevent the misuse of our models.\\xa0 For example, Imagen 3 and Veo\\ncreate more realistic imagery\\xa0 and videos, we must also\\nconsider how they might be\\xa0 misused to spread\\nmisinformation.\\xa0 To help, last year we introduced\\nSynthID, a tool that adds\\xa0 imperceptible watermarks to our\\nAI-generated images and audio so\\xa0 that they’re easier to identify.\\nToday, we’re expanding SynthID\\xa0 to two new modalities: Text and\\nvideo.\\xa0 These launches build on our\\nefforts to deploy\\xa0 state-of-the-art watermarking\\ncapabilities across modalities.\\xa0 Moving forward, we will keep\\nintegrating advances like\\xa0 watermarking and other emerging\\ntechniques, to secure our latest\\xa0 generations of Gemini, Imagine,\\nLyria, and Veo models.\\xa0 We’re also committed to working\\nwith the ecosystem with all of you\\xa0\\xa0 to help others build on the advances we\\'re making.\\nAnd in the coming months, we\\'ll be open-sourcing\\xa0\\xa0 SynthID text watermarking.\\nThis will be available in our\\xa0 updated Responsible Generative\\nAI Toolkit, which we created to\\xa0 make it easier for developers to\\nbuild AI responsibly.\\xa0 We\\'re also collaborating on\\xa0\\nC2PA, and we support C2PA,\\xa0 collaborating with Adobe,\\nMicrosoft, startups, and many\\xa0 others, to build and implement a\\nstandard that improves the\\xa0 transparency of digital media.\\nNow, let’s turn to the second\\xa0 and equally important part of\\nour responsible AI approach:\\xa0 How we’re building AI to benefit\\npeople and society.\\xa0 Today, our AI advances are\\nhelping to solve real-world\\xa0 problems, like accelerating the\\nwork of 1.8 million scientists\\xa0 in 190 countries who are using\\nAlphaFold to work on issues like\\xa0 neglected diseases.\\nHelping to predict floods in\\xa0 more than 80 countries.\\nAnd helping organizations, like\\xa0 the United Nations track progress on\\xa0\\nthe world\\'s 17 sustainable development\\xa0\\xa0 goals with Data Commons.\\nAnd now, generative AI is\\xa0 unlocking new ways for us to\\nmake the world’s information,\\xa0 and knowledge, universally\\naccessible and useful for\\xa0 learning.\\nBillions of people already use\\xa0 Google products to learn every day, and generative\\xa0\\nAI is opening up new possibilities, allowing us to\\xa0\\xa0 ask questions like, what if everyone\\neverywhere could have their own\\xa0 personal AI tutor, on any topic?\\nOr, what if every educator could\\xa0 have their own assistant in the\\nclassroom?\\xa0 Today marks a new chapter for\\nlearning and education at\\xa0 Google.\\nI am excited to introduce\\xa0 LearnLM, our new family of\\nmodels, based on Gemini, and\\xa0 fine-tuned for learning.\\nLearnLM is grounded in\\xa0 educational research, making\\nlearning experiences more\\xa0 personal and engaging.\\nAnd it’s coming to the products\\xa0 you use every day.\\nLike Search, Android, Gemini and YouTube.\\xa0 In fact, you\\'ve already seen LearnLM\\xa0\\non stage today when it helped Sameer\\xa0\\xa0 with his son\\'s homework on Android.\\nNow, let\\'s see how it works in\\xa0 the Gemini app.\\nEarlier, Sissie introduced Gems,\\xa0 custom versions of Gemini that\\ncan act as personal assistive\\xa0 experts on any topic.\\nWe are developing some pre-made\\xa0 Gems, which will be available in\\nthe Gemini App and web\\xa0 experience, including one called\\nLearning Coach.\\xa0 With Learning Coach, you can get\\nstep-by-step study guidance,\\xa0 along with helpful practice and\\nmemory techniques, designed to\\xa0 build understanding rather than\\njust give you the answer.\\xa0 Let’s say you’re a college\\nstudent studying for an upcoming\\xa0 biology exam.\\nIf you need a tip to remember\\xa0 the formula for photosynthesis,\\nLearning Coach can help.\\xa0 Learning Coach, along with other\\npre-made gems, will launch in\\xa0 Gemini in the coming months.\\nAnd you can imagine what\\xa0 features like Gemini Live can\\nunlock for learning.\\xa0 Another example is a new feature\\nin YouTube that uses LearnLM to\\xa0 make educational videos more\\ninteractive, allowing you to ask\\xa0 a clarifying question, get a\\nhelpful explanation, or take a\\xa0 quiz.\\nThis even works\\xa0\\xa0 for those long lectures or seminars, thanks\\xa0\\nto Gemini model\\'s long context capabilities.\\xa0 This feature in YouTube is\\nalready rolling out to select\\xa0 Android users.\\nAs we work to extend LearnLM\\xa0 beyond our own products, we are\\npartnering with experts and\\xa0 institutions like Columbia\\nTeachers College, Arizona State\\xa0 University and Khan Academy to\\ntest and improve the new\\xa0 capabilities in our models for\\nlearning.\\xa0 And we’ve collaborated with MIT\\nRAISE to develop an online\\xa0 course to help educators better\\nunderstand and use generative\\xa0 AI.\\nWe’re also working directly with\\xa0 educators to build more helpful\\ngenerative AI tools with Learn\\xa0 LM.\\nFor example, in Google\\xa0 Classroom, we’re drawing on the\\nadvances you’ve heard about\\xa0 today to develop new ways to simplify\\xa0\\nand improve lesson planning, and enable\\xa0\\xa0 teachers to tailor lessons and content to\\xa0\\nmeet the individual needs of their students.\\xa0 Standing here today makes me\\nthink back to my own time as an\\xa0 undergraduate.\\nThen, AI was considered\\xa0 speculative, far from any real\\nworld uses.\\xa0 Today, we can see how much is\\nalready real, how much it is\\xa0 already helping people, from\\ntheir everyday tasks to their\\xa0 most ambitious, productive and\\nimaginative endeavors, and how\\xa0 much more is still to come.\\nThis is what motivates us.\\xa0 I’m excited about what’s ahead\\nand what we’ll build with all of\\xa0 you.\\nBack to you, Sundar.\\xa0 [Applause].\\n>>SUNDAR PICHAI:\\xa0\\xa0 Thanks, James.\\nAll of this shows the important\\xa0 progress we’ve made, as we take\\na bold and responsible approach\\xa0 to making AI helpful for\\neveryone.\\xa0 Before we wrap, I have a feeling\\nthat someone out there might be\\xa0 counting how many times we’ve\\nmentioned AI today.\\xa0 [Laughter].\\nAnd since a big theme today has\\xa0 been letting Google do the work\\nfor you, we went ahead and\\xa0 counted, so that you don’t have\\nto. [Cheers and Applause].\\nThat might be a record in how\\xa0 many times someone has said AI.\\nI’m tempted to say it a few more\\xa0 times.\\nBut I won\\'t.\\xa0 Anyhow, this tally is more than just a punchline.\\xa0 It reflects something much\\ndeeper.\\xa0 We’ve been AI-first in our\\napproach for a long time.\\xa0 Our decades of research\\nleadership have pioneered many\\xa0 of the modern breakthroughs that\\npower AI progress, for us and\\xa0 for the industry.\\nOn top of that, we have\\xa0 world-leading infrastructure\\nbuilt for the AI Era,\\xa0 cutting-edge innovation in\\nSearch, now powered by Gemini,\\xa0 products that help at\\nextraordinary scale, including\\xa0 fifteen products with over half\\na billion users, and platforms\\xa0 that enable everyone, partners,\\ncustomers, creators, and all of\\xa0 you, to invent the future.\\nThis progress is only possible\\xa0 because of our incredible\\ndeveloper community.\\xa0 You are making it real, through\\nthe experiences you build every\\xa0 day.\\nSo, to everyone here in\\xa0 Shoreline and the millions more\\nwatching around the world,\\xa0 here’s to the possibilities\\nahead and creating them\\xa0 together.\\nThank you. [Cheers and Applause].\\n>> What does this remind you of?\\xa0 >> Cat.\\n>> Wow.\\xa0 >> Wow!\\n>> Okay!\\xa0 >> When all of these tools come\\ntogether, it\\'s a powerful\\xa0 combination.\\n>> It\\'s amazing.\\xa0 >> It\\'s amazing.\\nIt\\'s an entire suite of different\\xa0\\xa0 kinds of possibilities.\\n>> Hi.\\xa0 I\\'m Gemini.\\n>> What neighborhood do you\\xa0 think I\\'m in?\\n>> This appears to be the Kings\\xa0 Cross area of London.\\n>> Together we\\'re creating a new\\xa0 era.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LLM and Embeddings"
   ],
   "metadata": {
    "id": "njhwWarQjoZD"
   },
   "id": "njhwWarQjoZD"
  },
  {
   "cell_type": "code",
   "source": [
    "if COLAB:\n",
    "    from google.colab import userdata\n",
    "\n",
    "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
    "    LANGCHAIN_API_KEY = userdata.get('LANGCHAIN_API_KEY')\n",
    "else:\n",
    "    from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "    _ = load_dotenv(find_dotenv())\n",
    "    OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "    LANGCHAIN_API_KEY = os.getenv('LANGCHAIN_API_KEY')"
   ],
   "metadata": {
    "id": "SnAemSjmzh4T",
    "ExecuteTime": {
     "end_time": "2024-10-30T23:10:48.362315Z",
     "start_time": "2024-10-30T23:10:48.338036Z"
    }
   },
   "id": "SnAemSjmzh4T",
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "source": [
    "os.environ[\"LANGCHAIN_API_KEY\"] = LANGCHAIN_API_KEY\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = 'youtube-project'\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'  # enables tracing\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ],
   "metadata": {
    "id": "EYTn_8mV08Tv",
    "ExecuteTime": {
     "end_time": "2024-10-30T23:10:48.393717Z",
     "start_time": "2024-10-30T23:10:48.365363Z"
    }
   },
   "id": "EYTn_8mV08Tv",
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Helper Methods"
   ],
   "metadata": {
    "id": "pn1a-N4qjWES"
   },
   "id": "pn1a-N4qjWES"
  },
  {
   "cell_type": "code",
   "source": [
    "def clear_text(raw_text):\n",
    "    \"\"\"\n",
    "    Cleans the input text by removing unwanted characters and formatting.\n",
    "\n",
    "    Args:\n",
    "        raw_text (str): The input text to be cleaned.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned text.\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove bracketed content using regex\n",
    "    raw_text = re.sub(r\"\\[.*?\\]\", \"\", raw_text)\n",
    "\n",
    "    # Replace newline and non-breaking space characters with spaces\n",
    "    raw_text = raw_text.replace(\"\\n\", \" \").replace(\"\\xa0\", \" \")\n",
    "\n",
    "    # Remove speaker indicators using regex\n",
    "    raw_text = re.sub(r\">>.+?:\", \"\", raw_text)\n",
    "\n",
    "    # Remove all double Spaces\n",
    "    raw_text = raw_text.replace(\"  \", \" \")\n",
    "\n",
    "    # Remove doubled stops\n",
    "    raw_text = raw_text.replace(\". . \", \". \")\n",
    "\n",
    "    # Remove leading and trailing spaces\n",
    "    return raw_text.strip()"
   ],
   "metadata": {
    "id": "uLQzmwd7goAV",
    "ExecuteTime": {
     "end_time": "2024-10-30T23:10:48.424504Z",
     "start_time": "2024-10-30T23:10:48.414277Z"
    }
   },
   "id": "uLQzmwd7goAV",
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "source": [
    "def select_timestamps(sources):\n",
    "    \"\"\"\n",
    "    Selects relevant timestamps from a list of source documents.\n",
    "\n",
    "    Args:\n",
    "        sources (list): A list of source documents, each containing a metadata dictionary with a \"timestamp\" key.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of selected timestamps, sorted in ascending order and deduplicated.\n",
    "        Timestamps that are too close to the previous timestamp are removed to avoid redundancy.\n",
    "    \"\"\"\n",
    "\n",
    "    timestamps = [int(source.metadata[\"timestamp\"]) for source in sources]\n",
    "    timestamps = sorted(list(set(timestamps)))  # Deduplicate timestamps and sort it\n",
    "\n",
    "    # now remove timestamps which are too close to the timestamp before\n",
    "    result = []\n",
    "    threshold = 100\n",
    "    last_number = None  # Initialize to None to avoid skipping the first element\n",
    "\n",
    "    for number in timestamps:\n",
    "        if last_number is None or number - last_number >= threshold:\n",
    "            result.append(number)\n",
    "            last_number = number  # Update last_number for the next iteration\n",
    "\n",
    "    return result"
   ],
   "metadata": {
    "id": "eqa0LWRPEwSR",
    "ExecuteTime": {
     "end_time": "2024-10-30T23:10:48.464904Z",
     "start_time": "2024-10-30T23:10:48.443694Z"
    }
   },
   "id": "eqa0LWRPEwSR",
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "source": [
    "def prompt(title, chatter, question_text):\n",
    "    \"\"\"\n",
    "    Generates a prompt for a language model to answer questions about a YouTube video.\n",
    "\n",
    "    Args:\n",
    "        title (str): The title of the YouTube video.\n",
    "        question_text (str): The question to be answered.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted prompt string containing the video title and question.\n",
    "    \"\"\"\n",
    "\n",
    "    return f\"\"\"\n",
    "              You are a helpful and informative AI assistant. You are given a transcript of the video with the name \"{title}\". \n",
    "              {chatter} is asking questions. Please answer the following question, which comes after 'Question:'.\n",
    "              If the question cannot be answered using the information provided answer with \"I'm sorry {chatter}, I don't know\".\n",
    "              \n",
    "              Here are some examples, how you can answer the following question.\n",
    "              Examples:\n",
    "              What is the name of the Video?\n",
    "              You: {title}\n",
    "      \n",
    "              What's my name?\n",
    "              You: Your name is {chatter}.\n",
    "\n",
    "              Question: {question_text}\n",
    "              \"\"\""
   ],
   "metadata": {
    "id": "Q4aFqs0uhAHT",
    "ExecuteTime": {
     "end_time": "2024-10-30T23:35:36.508427Z",
     "start_time": "2024-10-30T23:35:36.493684Z"
    }
   },
   "id": "Q4aFqs0uhAHT",
   "outputs": [],
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "source": [
    "def ask_question_with_timestamp(title, chatter, question_text):\n",
    "    # Run the query to get the response and source documents\n",
    "    result = qa_chain.invoke(input=prompt(title, chatter, question_text), output_key=\"result\")\n",
    "    answer_text = clear_text(result[\"result\"])\n",
    "    sources = result[\"source_documents\"]\n",
    "\n",
    "    # define timestamps\n",
    "    timestamps = None\n",
    "    if \"I don't know.\" not in answer_text:\n",
    "        timestamps = select_timestamps(sources)\n",
    "\n",
    "    # Append timestamp information to the answer\n",
    "    return {\"answer\": answer_text, \"timestamps\": timestamps}"
   ],
   "metadata": {
    "id": "-CW6zfmK3AHm",
    "ExecuteTime": {
     "end_time": "2024-10-30T23:25:39.580357Z",
     "start_time": "2024-10-30T23:25:39.568362Z"
    }
   },
   "id": "-CW6zfmK3AHm",
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Langsmith"
   ],
   "metadata": {
    "id": "Az8Y7D9ysGFa"
   },
   "id": "Az8Y7D9ysGFa"
  },
  {
   "cell_type": "code",
   "source": [
    "# Initialize LangSmith client and tracer\n",
    "client = Client()\n",
    "tracer = LangChainTracer(client=client)"
   ],
   "metadata": {
    "id": "1iloT3S97JlQ",
    "ExecuteTime": {
     "end_time": "2024-10-30T23:10:48.571722Z",
     "start_time": "2024-10-30T23:10:48.552983Z"
    }
   },
   "id": "1iloT3S97JlQ",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "id": "8cf99d01001c74a0"
   },
   "cell_type": "markdown",
   "source": [
    "### FAISS Embedding"
   ],
   "id": "8cf99d01001c74a0"
  },
  {
   "metadata": {
    "id": "2823554457d3201",
    "outputId": "b14168ef-4b22-4b55-f089-03c73395d5d6",
    "ExecuteTime": {
     "end_time": "2024-10-30T23:10:48.619507Z",
     "start_time": "2024-10-30T23:10:48.589713Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the list to hold the chunks with metadata and the variables for current chunk\n",
    "chunks_with_metadata = []\n",
    "current_text = \"\"\n",
    "current_start = None\n",
    "\n",
    "# Maximum length for each chunk\n",
    "max_chunk_length = 1000\n",
    "\n",
    "# Iterate over each entry in raw_transcript\n",
    "for entry in raw_transcript:\n",
    "    # Set the start time for the first entry in the current chunk\n",
    "    if current_start is None:\n",
    "        current_start = entry['start']\n",
    "\n",
    "    # Check if adding the current text would exceed the max_chunk_length\n",
    "    if len(current_text) + len(entry['text']) + 1 > max_chunk_length:\n",
    "        # If it does, save the current chunk and reset the variables\n",
    "        chunks_with_metadata.append({'content': clear_text(current_text), 'timestamp': current_start})\n",
    "        current_text = \"\"\n",
    "        current_start = entry['start']\n",
    "\n",
    "    # Add the current text to the chunk with a space\n",
    "    current_text += entry['text'] + \" \"\n",
    "\n",
    "# After the loop, ensure any remaining text is added as a final chunk\n",
    "if current_text:\n",
    "    chunks_with_metadata.append({'content': clear_text(current_text), 'timestamp': current_start})\n",
    "\n",
    "# Print the average length of the generated chunks\n",
    "average_length = sum(len(entry['content']) for entry in chunks_with_metadata) / len(chunks_with_metadata)\n",
    "print(\"Average length:\", int(average_length))\n",
    "print(\"Chunks:\", len(chunks_with_metadata))"
   ],
   "id": "2823554457d3201",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length: 911\n",
      "Chunks: 93\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "source": [
    "# Generate embeddings for each chunk\n",
    "embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY)\n",
    "texts = [chunk[\"content\"] for chunk in chunks_with_metadata]\n",
    "metadata = [{\"timestamp\": chunk[\"timestamp\"]} for chunk in chunks_with_metadata]\n",
    "\n",
    "vectorstore = FAISS.from_texts(texts, embeddings, metadatas=metadata)"
   ],
   "metadata": {
    "id": "CJRC5aGbj76n",
    "ExecuteTime": {
     "end_time": "2024-10-30T23:17:11.899070Z",
     "start_time": "2024-10-30T23:17:09.509746Z"
    }
   },
   "id": "CJRC5aGbj76n",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "id": "e933c66dbd091659"
   },
   "cell_type": "markdown",
   "source": [
    "### Model with Memory"
   ],
   "id": "e933c66dbd091659"
  },
  {
   "cell_type": "code",
   "source": [
    "# Initialize the language model\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Set up chat memory\n",
    "conversational_memory = ConversationBufferWindowMemory(\n",
    "    memory_key='youtube_project_history',\n",
    "    k=3,\n",
    "    return_messages=True,\n",
    "    output_key='result'\n",
    ")\n",
    "\n",
    "# Set up the RetrievalQA chain with vectorstore and tracer for LangSmith logging\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    memory=conversational_memory,\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    callbacks=[tracer]\n",
    ")"
   ],
   "metadata": {
    "id": "-g6HLAoCj-q-",
    "ExecuteTime": {
     "end_time": "2024-10-30T23:25:52.478905Z",
     "start_time": "2024-10-30T23:25:52.339986Z"
    }
   },
   "id": "-g6HLAoCj-q-",
   "outputs": [],
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "source": [
    "# Example question\n",
    "question = \"Can you give me a summarize of the video\"\n",
    "question = \"What is the name of the Video?\"\n",
    "answer = ask_question_with_timestamp(video_title, chatter, question)\n",
    "print(answer)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Klx6fJ6SS07",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1730299585595,
     "user_tz": -60,
     "elapsed": 2277,
     "user": {
      "displayName": "Fabian Hieber",
      "userId": "06681858431281640040"
     }
    },
    "outputId": "2f11bcae-fd07-4bc9-8b88-6d625b2147de",
    "ExecuteTime": {
     "end_time": "2024-10-30T23:36:54.483183Z",
     "start_time": "2024-10-30T23:36:52.185828Z"
    }
   },
   "id": "4Klx6fJ6SS07",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': \"I'm sorry Fabian, I don't know.\", 'timestamps': None}\n"
     ]
    }
   ],
   "execution_count": 53
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
